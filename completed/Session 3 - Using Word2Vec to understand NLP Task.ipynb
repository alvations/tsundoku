{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSRJr6FMHAIB"
   },
   "outputs": [],
   "source": [
    "!pip install tsundoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3sFD_Q-HAIE"
   },
   "outputs": [],
   "source": [
    "!pip install sklearn torch tqdm nltk lazyme ansi requests gensim\n",
    "!python -m nltk.downloader movie_reviews punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMfIhsm9HAIH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "from tsundoku.word2vec_hints import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcNxTyz-HAII"
   },
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "- <a href=\"#section-3-0\">**3.0. Data Preparation**</a>\n",
    "  - <a href=\"#section-3-0-1\">3.0.1. *Vocabulary*</a>\n",
    "    - <a href=\"#section-3-0-1-a\"> Pet Peeve: using `gensim`</a>\n",
    "  - <a href=\"#section-3-0-2\">3.0.2. *Dataset*</a>  (<a href=\"#section-3-0-2-hints\">Hints</a>)\n",
    "    - <a href=\"#section-3-0-2-return-dict\">Return `dict` in `__getitem__()`</a>\n",
    "    - <a href=\"#section-3-0-2-labeleddata\">Try `LabeledDataset`</a>\n",
    "<br><br>\n",
    "- <a href=\"#section-3-1\">**3.1. Word2Vec from Scratch**</a>\n",
    "  - <a href=\"#section-3-1-1\">3.1.1. *CBOW*</a>\n",
    "  - <a href=\"#section-3-1-2\">3.1.2. *Skipgram*</a>\n",
    "  - <a href=\"#section-3-1-3\">3.1.3. *Word2Vec Dataset*</a> (<a href=\"#section-3-1-3-hint\">Hints</a>)\n",
    "  - <a href=\"#section-3-1-4-hint\">3.1.4. *Train a CBOW model*</a>\n",
    "    - <a href=\"#section-3-1-4-fill-cbow\">The CBOW model</a>\n",
    "    - <a href=\"#section-3-1-4-train-cbow\">Train the model (*for real*)</a>\n",
    "    - <a href=\"#section-3-1-4-evaluate-cbow\">Evaluate the model</a>\n",
    "    - <a href=\"#section-3-1-4-load-model\">Load model at specific epoch</a>\n",
    "  - <a href=\"#section-3-1-5\">3.1.5. *Train a Skipgram model*</a>\n",
    "    - <a href=\"#section-3-1-5-forward\">Take a closer look at `forward()`</a>\n",
    "    - <a href=\"#section-3-1-5-train\">Train the model (*for real*)</a>\n",
    "    - <a href=\"section-3-1-5-evaluate\">Evaluate the model</a>\n",
    "  - <a href=\"#section-3-1-6\">3.1.6. *Loading Pre-trained Embeddings*</a>\n",
    "    - <a href=\"#section-3-1-6-vocab\">Override the Embedding vocabulary</a>\n",
    "    - <a href=\"#section-3-1-6-pretrained\">Override the Embedding weights</a>\n",
    "    - <a href=\"#section-3-1-6-eval-skipgram\">Evaluate on the Skipgram task</a>\n",
    "    - <a href=\"#section-3-1-6-eval-cbow\">Evaluate on the CBOW task</a>\n",
    "    - <a href=\"#section-3-1-6-unfreeze-finetune\">Unfreeeze and finetune</a>\n",
    "    - <a href=\"#section-3-1-6-reval-cbow\">Re-evaluate on the CBOW task</a>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgogNA_4HAIK"
   },
   "source": [
    "<a id=\"section-3-0\"></a>\n",
    "# 3.0. Data Preparation\n",
    "\n",
    "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
    "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
    "\n",
    "There are already several other libraries that help with loading text datasets, e.g. \n",
    "\n",
    " - FastAI https://docs.fast.ai/text.data.html\n",
    " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
    " - Torch Text https://github.com/pytorch/text#data\n",
    " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
    " - SpaCy https://github.com/explosion/thinc\n",
    " \n",
    "\n",
    "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBuDtrsfHAIK"
   },
   "source": [
    "<a id=\"section-3-0-1\"></a>\n",
    "## 3.0.1  Vocabulary\n",
    "\n",
    "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRRKjvgdHAIL"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdW-wU64HAIM"
   },
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtDJBSPNHAIO"
   },
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
    "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
    "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
    "the null hypothesis will never be true. Moreover, where there is enough\n",
    "data, we shall (almost) always be able to establish that it is not true. In\n",
    "corpus studies, we frequently do have enough data, so the fact that a relation \n",
    "between two phenomena is demonstrably non-random, does not support the inference \n",
    "that it is not arbitrary. We present experimental evidence\n",
    "of how arbitrary associations between word frequencies and corpora are\n",
    "systematically non-random. We review literature in which hypothesis testing \n",
    "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
    "\n",
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "uniq_tokens = set(chain(*tokenized_text))\n",
    "\n",
    "vocab = {}   # Assign indices to every word.\n",
    "idx2tok = {} # Also keep an dict of index to words.\n",
    "for i, token in enumerate(uniq_tokens):\n",
    "    vocab[token] = i\n",
    "    idx2tok[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_QntiwpHAIR",
    "outputId": "c15a5a33-21e9-46f3-e782-2028bc0e3de0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'present': 0,\n",
       " 'does': 1,\n",
       " 'support': 2,\n",
       " '.': 3,\n",
       " 'linguistic': 4,\n",
       " 'has': 5,\n",
       " 'so': 6,\n",
       " 'have': 7,\n",
       " 'look': 8,\n",
       " 'phenomena': 9,\n",
       " 'establish': 10,\n",
       " 'statistical': 11,\n",
       " 'shall': 12,\n",
       " 'experimental': 13,\n",
       " 'or': 14,\n",
       " 'words': 15,\n",
       " 'to': 16,\n",
       " 'language': 17,\n",
       " 'results': 18,\n",
       " 'of': 19,\n",
       " 'null': 20,\n",
       " 'when': 21,\n",
       " 'inference': 22,\n",
       " 'show': 23,\n",
       " 'it': 24,\n",
       " 'frequencies': 25,\n",
       " 'testing': 26,\n",
       " 'enough': 27,\n",
       " 'randomness': 28,\n",
       " 'and': 29,\n",
       " 'two': 30,\n",
       " 'will': 31,\n",
       " ',': 32,\n",
       " 'randomly': 33,\n",
       " 'that': 34,\n",
       " 'in': 35,\n",
       " 'at': 36,\n",
       " 'studies': 37,\n",
       " 'the': 38,\n",
       " 'demonstrably': 39,\n",
       " 'there': 40,\n",
       " 'do': 41,\n",
       " '(': 42,\n",
       " 'is': 43,\n",
       " 'almost': 44,\n",
       " 'always': 45,\n",
       " 'relation': 46,\n",
       " 'corpus': 47,\n",
       " 'non-random': 48,\n",
       " 'be': 49,\n",
       " 'used': 50,\n",
       " 'users': 51,\n",
       " 'posits': 52,\n",
       " 'associations': 53,\n",
       " 'been': 54,\n",
       " 'are': 55,\n",
       " 'often': 56,\n",
       " 'arbitrary': 57,\n",
       " 'unhelpful': 58,\n",
       " 'true': 59,\n",
       " 'able': 60,\n",
       " 'choose': 61,\n",
       " 'systematically': 62,\n",
       " 'between': 63,\n",
       " 'essentially': 64,\n",
       " 'uses': 65,\n",
       " 'misleading': 66,\n",
       " 'where': 67,\n",
       " ')': 68,\n",
       " 'review': 69,\n",
       " 'never': 70,\n",
       " 'a': 71,\n",
       " 'we': 72,\n",
       " 'led': 73,\n",
       " 'which': 74,\n",
       " 'data': 75,\n",
       " 'moreover': 76,\n",
       " 'evidence': 77,\n",
       " 'fact': 78,\n",
       " 'hence': 79,\n",
       " 'frequently': 80,\n",
       " 'how': 81,\n",
       " 'hypothesis': 82,\n",
       " 'not': 83,\n",
       " 'corpora': 84,\n",
       " 'word': 85,\n",
       " 'literature': 86}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saDeNLfqHAIV",
    "outputId": "d2ee5161-86bf-4b1c-c3a5-6cd3939d9e27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the index of the word 'corpora'\n",
    "vocab['non-random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kY7UzMSRHAIX",
    "outputId": "4d2e48cf-2ed7-496a-e301-699d75db9680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[17, 51, 70, 61, 15, 33, 32, 29, 17, 43, 64, 48, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indexed representation of the first sentence.\n",
    "\n",
    "sent0 = tokenized_text[0]\n",
    "print(sent0)\n",
    "[vocab[token] for token in sent0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZOa3f3RHAIa"
   },
   "source": [
    "<a id=\"section-3-0-1-a\"></a>\n",
    "\n",
    "### Pet Peeve (Gensim)\n",
    "\n",
    "I (Liling) don't really like to write my own vectorizer the `gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
    "\n",
    "Using `gensim`, I would have written the above as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pL36b-azHAId"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "vocab = Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uegCsbqqHAIg",
    "outputId": "e5e72187-af67-4cbf-9f1e-05ade01cf946",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ',',\n",
       " 1: '.',\n",
       " 2: 'and',\n",
       " 3: 'choose',\n",
       " 4: 'essentially',\n",
       " 5: 'is',\n",
       " 6: 'language',\n",
       " 7: 'never',\n",
       " 8: 'non-random',\n",
       " 9: 'randomly',\n",
       " 10: 'users',\n",
       " 11: 'words',\n",
       " 12: 'a',\n",
       " 13: 'hypothesis',\n",
       " 14: 'null',\n",
       " 15: 'posits',\n",
       " 16: 'randomness',\n",
       " 17: 'statistical',\n",
       " 18: 'testing',\n",
       " 19: 'uses',\n",
       " 20: 'which',\n",
       " 21: 'at',\n",
       " 22: 'be',\n",
       " 23: 'corpora',\n",
       " 24: 'hence',\n",
       " 25: 'in',\n",
       " 26: 'linguistic',\n",
       " 27: 'look',\n",
       " 28: 'phenomena',\n",
       " 29: 'the',\n",
       " 30: 'true',\n",
       " 31: 'we',\n",
       " 32: 'when',\n",
       " 33: 'will',\n",
       " 34: '(',\n",
       " 35: ')',\n",
       " 36: 'able',\n",
       " 37: 'almost',\n",
       " 38: 'always',\n",
       " 39: 'data',\n",
       " 40: 'enough',\n",
       " 41: 'establish',\n",
       " 42: 'it',\n",
       " 43: 'moreover',\n",
       " 44: 'not',\n",
       " 45: 'shall',\n",
       " 46: 'that',\n",
       " 47: 'there',\n",
       " 48: 'to',\n",
       " 49: 'where',\n",
       " 50: 'arbitrary',\n",
       " 51: 'between',\n",
       " 52: 'corpus',\n",
       " 53: 'demonstrably',\n",
       " 54: 'do',\n",
       " 55: 'does',\n",
       " 56: 'fact',\n",
       " 57: 'frequently',\n",
       " 58: 'have',\n",
       " 59: 'inference',\n",
       " 60: 'relation',\n",
       " 61: 'so',\n",
       " 62: 'studies',\n",
       " 63: 'support',\n",
       " 64: 'two',\n",
       " 65: 'are',\n",
       " 66: 'associations',\n",
       " 67: 'evidence',\n",
       " 68: 'experimental',\n",
       " 69: 'frequencies',\n",
       " 70: 'how',\n",
       " 71: 'of',\n",
       " 72: 'present',\n",
       " 73: 'systematically',\n",
       " 74: 'word',\n",
       " 75: 'been',\n",
       " 76: 'has',\n",
       " 77: 'led',\n",
       " 78: 'literature',\n",
       " 79: 'misleading',\n",
       " 80: 'often',\n",
       " 81: 'or',\n",
       " 82: 'results',\n",
       " 83: 'review',\n",
       " 84: 'show',\n",
       " 85: 'unhelpful',\n",
       " 86: 'used'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the key-value order is different of gensim from the native Python's\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEcAMbOLHAIo",
    "outputId": "16346ac8-4771-4c71-adf8-5fc77d83faaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token2id['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7CQ2gbqHAIs",
    "outputId": "f5855372-e4f3-4236-b910-72a342dae641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.doc2idx(sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8_Dg_tMHAIv"
   },
   "source": [
    "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZGA2_RLHAIw"
   },
   "source": [
    "<a id=\"section-3-0-2\"></a>\n",
    "\n",
    "# 3.0.2 Dataset\n",
    "\n",
    "Lets try creating a `torch.utils.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLDrY4UDHAIx"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.vocab = Dictionary(tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        # Hint: You want to return a vectorized sentence here.\n",
    "        return {'x': self.vectorize(self.sents[index])}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rD0LtXVSHAI0"
   },
   "source": [
    "<a id=\"section-3-0-2-hints\"></a>\n",
    "## Hints to the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYmg5cqJHAI1"
   },
   "outputs": [],
   "source": [
    "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
    "#hint_dataset_vectorize()\n",
    "#code_text_dataset_vectorize()\n",
    "\n",
    "# Option 2: \"I give up just, run the code for me\" \n",
    "# Uncomment the next two lines, if you really gave up... \n",
    "#full_code_text_dataset_vectorize()\n",
    "#from tsundoku.word2vec import Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3trRgP9LHAI2",
    "outputId": "4c153e24-81ef-4abb-c967-0ffa401020e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'present', 'experimental', 'evidence', 'of', 'how', 'arbitrary', 'associations', 'between', 'word', 'frequencies', 'and', 'corpora', 'are', 'systematically', 'non-random', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xHVvYSpHAI4"
   },
   "outputs": [],
   "source": [
    "text_dataset = Text(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfupnp_cHAI5",
    "outputId": "9fbe524d-3791-4925-b6a9-6de0f8d6a09a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[0] # First sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34a_siOHHAI9"
   },
   "source": [
    "<a id=\"section-3-0-2-return-dict\"></a>\n",
    "\n",
    "### Return `dict` in `__getitem__()`\n",
    "\n",
    "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
    "\n",
    "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3XTAsPGHAI-"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LabeledText(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.labels = labels # Sentence level labels.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        return {'x': self.vectorize(self.sents[index]), \n",
    "                'y': self.labels[index]}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93706NQMHAJB"
   },
   "source": [
    "<a id=\"section-3-0-2-labeleddata\"></a>\n",
    "\n",
    "### Lets try the `LabeledDataset` on a movie review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZc9TAkaHAJD"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dd2lVKPvHAJI",
    "outputId": "848c0622-d6ea-448c-d108-f7447ac01c06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:17<00:00, 115.20it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for fileid in tqdm(movie_reviews.fileids()):\n",
    "    label = fileid.split('/')[0]\n",
    "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
    "    documents.append(doc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6v0H41lDHAJK",
    "outputId": "c33bb1e5-50d2-4c2d-d759-3fd2eccab2c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    }
   ],
   "source": [
    "print(documents[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONbM7Rz7HAJQ"
   },
   "outputs": [],
   "source": [
    "labeled_dataset = LabeledText(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNCIdsKtHAJT",
    "outputId": "a6add92a-d5fb-45a3-b23b-4fa073b15eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': [243, 17, 314, 294, 77, 140, 307, 20, 68, 237, 6, 97, 34, 299, 98, 8, 302, 135, 167, 33, 22, 8, 226, 220, 297, 145, 87, 6, 60, 158, 136, 74, 307, 262, 157, 165, 153, 179, 6, 34, 149, 214, 8, 333, 2, 297, 82, 18, 326, 297, 204, 34, 19, 280, 19, 124, 230, 8, 8, 8, 79, 17, 20, 199, 204, 129, 297, 294, 133, 296, 311, 225, 20, 322, 75, 164, 6, 60, 245, 169, 165, 20, 322, 46, 234, 8, 337, 168, 333, 188, 304, 253, 33, 108, 148, 226, 307, 345, 6, 272, 163, 132, 37, 122, 337, 42, 307, 59, 297, 201, 6, 196, 341, 348, 152, 34, 290, 4, 185, 156, 1, 195, 5, 6, 60, 300, 38, 142, 34, 46, 328, 220, 189, 28, 315, 220, 122, 6, 34, 301, 128, 173, 86, 208, 276, 304, 226, 76, 8, 302, 263, 307, 150, 293, 304, 246, 209, 72, 6, 60, 113, 169, 295, 8, 277, 333, 38, 297, 248, 341, 297, 204, 18, 331, 6, 170, 186, 247, 168, 296, 169, 2, 271, 309, 172, 8, 169, 282, 221, 19, 216, 19, 60, 299, 95, 167, 304, 19, 116, 19, 342, 165, 337, 347, 6, 40, 33, 43, 194, 6, 150, 215, 164, 333, 2, 141, 225, 8, 300, 38, 96, 6, 300, 38, 64, 70, 45, 130, 297, 81, 6, 300, 38, 229, 339, 183, 180, 297, 81, 6, 300, 38, 286, 36, 6, 300, 38, 91, 6, 300, 38, 20, 184, 220, 65, 260, 6, 300, 38, 308, 220, 330, 303, 296, 147, 6, 34, 203, 220, 169, 168, 271, 217, 114, 8, 218, 163, 240, 92, 208, 198, 312, 307, 317, 20, 121, 110, 218, 34, 299, 6, 60, 335, 28, 169, 93, 168, 137, 190, 297, 259, 69, 231, 34, 231, 26, 6, 163, 135, 175, 220, 117, 320, 25, 20, 338, 6, 337, 168, 304, 121, 2, 54, 247, 8, 169, 2, 219, 143, 304, 53, 261, 307, 155, 6, 60, 169, 265, 307, 325, 307, 155, 169, 71, 319, 170, 123, 125, 200, 8, 34, 92, 302, 187, 303, 106, 6, 305, 228, 108, 103, 6, 165, 297, 192, 18, 217, 251, 8, 297, 256, 236, 168, 296, 297, 39, 34, 163, 57, 89, 225, 127, 180, 304, 6, 277, 329, 24, 120, 203, 220, 169, 230, 61, 297, 146, 244, 6, 277, 28, 220, 297, 287, 25, 296, 86, 281, 307, 187, 20, 182, 55, 220, 266, 6, 60, 169, 284, 86, 208, 297, 187, 297, 121, 28, 296, 202, 106, 8, 163, 144, 297, 58, 181, 341, 205, 180, 304, 168, 296, 347, 268, 31, 187, 292, 296, 297, 43, 168, 19, 167, 169, 19, 108, 51, 302, 38, 138, 297, 261, 238, 307, 104, 348, 342, 220, 316, 8, 163, 191, 6, 269, 193, 257, 254, 44, 130, 324, 129, 21, 11, 200, 306, 297, 204, 168, 173, 241, 178, 0, 0, 224, 6, 329, 135, 169, 8, 8, 8, 300, 38, 239, 66, 153, 34, 329, 92, 208, 176, 339, 302, 38, 8, 92, 329, 251, 210, 307, 262, 169, 231, 34, 231, 26, 18, 162, 21, 139, 321, 88, 260, 222, 131, 166, 167, 28, 220, 297, 287, 141, 94, 165, 297, 204, 18, 35, 6, 297, 289, 310, 304, 121, 44, 130, 170, 90, 34, 67, 169, 320, 298, 6, 34, 169, 270, 8, 300, 197, 3, 50, 20, 246, 83, 294, 199, 204, 165, 154, 279, 6, 60, 163, 144, 19, 297, 291, 19, 84, 296, 313, 169, 167, 20, 206, 323, 341, 182, 100, 6, 343, 187, 202, 266, 8, 297, 23, 38, 246, 142, 129, 297, 203, 236, 6, 30, 332, 52, 173, 264, 307, 47, 242, 297, 111, 259, 63, 296, 151, 86, 165, 32, 48, 6, 227, 165, 20, 212, 211, 8, 60, 207, 54, 177, 140, 230, 307, 257, 6, 339, 159, 153, 233, 306, 297, 107, 121, 6, 34, 24, 149, 347, 118, 153, 63, 2, 318, 8, 232, 6, 297, 121, 93, 208, 283, 49, 169, 93, 208, 105, 6, 169, 2, 73, 6, 169, 250, 112, 34, 169, 119, 246, 252, 129, 203, 220, 170, 255, 6, 85, 20, 246, 75, 102, 34, 115, 307, 28, 220, 297, 78, 296, 62, 51, 169, 8, 223, 6, 34, 61, 297, 327, 6, 304, 168, 217, 20, 160, 228, 294, 275, 126, 8, 8, 8, 169, 2, 173, 235, 307, 183, 296, 327, 49, 278, 168, 35, 41, 296, 297, 134, 168, 284, 161, 341, 297, 174, 8, 169, 29, 344, 249, 314, 346, 27, 34, 149, 50, 273, 225, 297, 267, 109, 272, 8, 334, 8, 8, 8, 274, 169, 0, 336, 2, 171, 70, 130, 18, 20, 213, 220, 101, 288, 12, 4, 14, 5, 7, 56, 340, 10, 4, 14, 5, 7, 297, 80, 4, 16, 5, 7, 297, 80, 17, 258, 4, 13, 5, 7, 185, 156, 4, 9, 5, 7, 195, 4, 9, 5, 7, 297, 229, 4, 16, 5, 7, 285, 220, 99, 4, 15, 5], 'y': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(labeled_dataset[0])  # First review in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BE5n1Bn6HAJV",
    "outputId": "04490253-43dc-4d6b-b3b7-b07891667b1b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243, 17, 314, 294, 77, 140, 307, 20, 68, 237, 6, 97, 34, 299, 98, 8, 302, 135, 167, 33, 22, 8, 226, 220, 297, 145, 87, 6, 60, 158, 136, 74, 307, 262, 157, 165, 153, 179, 6, 34, 149, 214, 8, 333, 2, 297, 82, 18, 326, 297, 204, 34, 19, 280, 19, 124, 230, 8, 8, 8, 79, 17, 20, 199, 204, 129, 297, 294, 133, 296, 311, 225, 20, 322, 75, 164, 6, 60, 245, 169, 165, 20, 322, 46, 234, 8, 337, 168, 333, 188, 304, 253, 33, 108, 148, 226, 307, 345, 6, 272, 163, 132, 37, 122, 337, 42, 307, 59, 297, 201, 6, 196, 341, 348, 152, 34, 290, 4, 185, 156, 1, 195, 5, 6, 60, 300, 38, 142, 34, 46, 328, 220, 189, 28, 315, 220, 122, 6, 34, 301, 128, 173, 86, 208, 276, 304, 226, 76, 8, 302, 263, 307, 150, 293, 304, 246, 209, 72, 6, 60, 113, 169, 295, 8, 277, 333, 38, 297, 248, 341, 297, 204, 18, 331, 6, 170, 186, 247, 168, 296, 169, 2, 271, 309, 172, 8, 169, 282, 221, 19, 216, 19, 60, 299, 95, 167, 304, 19, 116, 19, 342, 165, 337, 347, 6, 40, 33, 43, 194, 6, 150, 215, 164, 333, 2, 141, 225, 8, 300, 38, 96, 6, 300, 38, 64, 70, 45, 130, 297, 81, 6, 300, 38, 229, 339, 183, 180, 297, 81, 6, 300, 38, 286, 36, 6, 300, 38, 91, 6, 300, 38, 20, 184, 220, 65, 260, 6, 300, 38, 308, 220, 330, 303, 296, 147, 6, 34, 203, 220, 169, 168, 271, 217, 114, 8, 218, 163, 240, 92, 208, 198, 312, 307, 317, 20, 121, 110, 218, 34, 299, 6, 60, 335, 28, 169, 93, 168, 137, 190, 297, 259, 69, 231, 34, 231, 26, 6, 163, 135, 175, 220, 117, 320, 25, 20, 338, 6, 337, 168, 304, 121, 2, 54, 247, 8, 169, 2, 219, 143, 304, 53, 261, 307, 155, 6, 60, 169, 265, 307, 325, 307, 155, 169, 71, 319, 170, 123, 125, 200, 8, 34, 92, 302, 187, 303, 106, 6, 305, 228, 108, 103, 6, 165, 297, 192, 18, 217, 251, 8, 297, 256, 236, 168, 296, 297, 39, 34, 163, 57, 89, 225, 127, 180, 304, 6, 277, 329, 24, 120, 203, 220, 169, 230, 61, 297, 146, 244, 6, 277, 28, 220, 297, 287, 25, 296, 86, 281, 307, 187, 20, 182, 55, 220, 266, 6, 60, 169, 284, 86, 208, 297, 187, 297, 121, 28, 296, 202, 106, 8, 163, 144, 297, 58, 181, 341, 205, 180, 304, 168, 296, 347, 268, 31, 187, 292, 296, 297, 43, 168, 19, 167, 169, 19, 108, 51, 302, 38, 138, 297, 261, 238, 307, 104, 348, 342, 220, 316, 8, 163, 191, 6, 269, 193, 257, 254, 44, 130, 324, 129, 21, 11, 200, 306, 297, 204, 168, 173, 241, 178, 0, 0, 224, 6, 329, 135, 169, 8, 8, 8, 300, 38, 239, 66, 153, 34, 329, 92, 208, 176, 339, 302, 38, 8, 92, 329, 251, 210, 307, 262, 169, 231, 34, 231, 26, 18, 162, 21, 139, 321, 88, 260, 222, 131, 166, 167, 28, 220, 297, 287, 141, 94, 165, 297, 204, 18, 35, 6, 297, 289, 310, 304, 121, 44, 130, 170, 90, 34, 67, 169, 320, 298, 6, 34, 169, 270, 8, 300, 197, 3, 50, 20, 246, 83, 294, 199, 204, 165, 154, 279, 6, 60, 163, 144, 19, 297, 291, 19, 84, 296, 313, 169, 167, 20, 206, 323, 341, 182, 100, 6, 343, 187, 202, 266, 8, 297, 23, 38, 246, 142, 129, 297, 203, 236, 6, 30, 332, 52, 173, 264, 307, 47, 242, 297, 111, 259, 63, 296, 151, 86, 165, 32, 48, 6, 227, 165, 20, 212, 211, 8, 60, 207, 54, 177, 140, 230, 307, 257, 6, 339, 159, 153, 233, 306, 297, 107, 121, 6, 34, 24, 149, 347, 118, 153, 63, 2, 318, 8, 232, 6, 297, 121, 93, 208, 283, 49, 169, 93, 208, 105, 6, 169, 2, 73, 6, 169, 250, 112, 34, 169, 119, 246, 252, 129, 203, 220, 170, 255, 6, 85, 20, 246, 75, 102, 34, 115, 307, 28, 220, 297, 78, 296, 62, 51, 169, 8, 223, 6, 34, 61, 297, 327, 6, 304, 168, 217, 20, 160, 228, 294, 275, 126, 8, 8, 8, 169, 2, 173, 235, 307, 183, 296, 327, 49, 278, 168, 35, 41, 296, 297, 134, 168, 284, 161, 341, 297, 174, 8, 169, 29, 344, 249, 314, 346, 27, 34, 149, 50, 273, 225, 297, 267, 109, 272, 8, 334, 8, 8, 8, 274, 169, 0, 336, 2, 171, 70, 130, 18, 20, 213, 220, 101, 288, 12, 4, 14, 5, 7, 56, 340, 10, 4, 14, 5, 7, 297, 80, 4, 16, 5, 7, 297, 80, 17, 258, 4, 13, 5, 7, 185, 156, 4, 9, 5, 7, 195, 4, 9, 5, 7, 297, 229, 4, 16, 5, 7, 285, 220, 99, 4, 15, 5]\n"
     ]
    }
   ],
   "source": [
    "print(labeled_dataset[0]['x'])  # First review in vectorized index format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYKW3HTAHAJe",
    "outputId": "a56569b3-e66d-4718-aff9-c4801f79626f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "print(labeled_dataset[0]['y'])  # Label of the first review in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAmO-CfpHAJi"
   },
   "source": [
    "<a id=\"section-3-1\"></a>\n",
    "\n",
    "# 3.1 Word2Vec Training\n",
    "\n",
    "Word2Vec has two training variants:\n",
    "\n",
    " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
    " - **Skip-grams**: Predict context words given center word.\n",
    "  \n",
    "Visually, they look like this:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8s5BWXEHAJj"
   },
   "source": [
    "Fig. 1. The skip-gram model. Both the input vector xx and the output yy are one-hot encoded word representations. <br>The hidden layer is the word embedding of size NN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3Tott_jHAKY"
   },
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-skip-gram.png\" width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGbH-mS-HAKa"
   },
   "source": [
    "Fig. 2. The CBOW model. Word vectors of multiple context words are averaged to get a fixed-length vector as in the hidden layer. Other symbols have the same meanings as in Fig 1.\n",
    "\n",
    "(Pretty network images above are from [https://lilianweng.github.io](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#context-based-continuous-bag-of-words-cbow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSckJrhpHAKc"
   },
   "source": [
    "<a id=\"section-3-1-1\"></a>\n",
    "\n",
    "## 3.1.1. CBOW\n",
    "\n",
    "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAPrZ_tsHAKt",
    "outputId": "3c743c14-6a77-4c86-d24a-2b3919bd558c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lazyme import per_window, per_chunk\n",
    "\n",
    "xx =[1,2,3,4,5, 6]\n",
    "list(per_window(xx, n=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3, 4), (5, 6, None, None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(per_chunk(xx, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FV3-eKJgHAK0"
   },
   "outputs": [],
   "source": [
    "def per_window(sequence, n=1):\n",
    "    \"\"\"\n",
    "    From http://stackoverflow.com/q/42220614/610569\n",
    "        >>> list(per_window([1,2,3,4], n=2))\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "        >>> list(per_window([1,2,3,4], n=3))\n",
    "        [(1, 2, 3), (2, 3, 4)]\n",
    "    \"\"\"\n",
    "    start, stop = 0, n\n",
    "    seq = list(sequence)\n",
    "    while stop <= len(seq):\n",
    "        yield seq[start:stop]\n",
    "        start += 1\n",
    "        stop += 1\n",
    "\n",
    "def cbow_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1\n",
    "    for window in per_window(tokens, n):\n",
    "        target = window.pop(window_size)\n",
    "        yield window, target   # X = window ; Y = target. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzyORy8cHALB"
   },
   "outputs": [],
   "source": [
    "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
    "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNEkMM0DHALJ",
    "outputId": "6b8ce2b4-dfc9-426a-9e1a-a17da38c27b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'choose', 'words'], 'never'),\n",
       " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
       " (['never', 'choose', 'randomly', ','], 'words'),\n",
       " (['choose', 'words', ',', 'and'], 'randomly'),\n",
       " (['words', 'randomly', 'and', 'language'], ','),\n",
       " (['randomly', ',', 'language', 'is'], 'and'),\n",
       " ([',', 'and', 'is', 'essentially'], 'language'),\n",
       " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
       " (['language', 'is', 'non-random', '.'], 'essentially')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-6qY-gaHALm",
    "outputId": "2965aa31-98d0-421a-fd12-cd5c510b5e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
       " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
       " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
       " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
       " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
       " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
       " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ljJ20HHFHAL-"
   },
   "source": [
    "<a id=\"section-3-1-2\"></a>\n",
    "\n",
    "## 3.1.2. Skipgram\n",
    "\n",
    "Skipgram training windows through the sentence and pictures out the center word as the input `X` and the context words as the outputs `Y`, additionally, it will randommly sample words not in the window as **negative samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3fPMkoZHAL_"
   },
   "outputs": [],
   "source": [
    "def skipgram_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1 \n",
    "    for i, window in enumerate(per_window(tokens, n)):\n",
    "        target = window.pop(window_size)\n",
    "        # Generate positive samples.\n",
    "        for context_word in window:\n",
    "            yield target, context_word, 1\n",
    "        # Generate negative samples.\n",
    "        for _ in range(n-1):\n",
    "            leftovers = tokens[:i] + tokens[i+n:]\n",
    "            yield target, random.choice(leftovers), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-esIo3JlHAME",
    "outputId": "744f2af7-d053-4846-e303-c68893c21e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('never', 'language', 1),\n",
       " ('never', 'users', 1),\n",
       " ('never', 'choose', 1),\n",
       " ('never', 'words', 1),\n",
       " ('never', 'language', 0),\n",
       " ('never', 'non-random', 0),\n",
       " ('never', '.', 0),\n",
       " ('never', 'and', 0),\n",
       " ('choose', 'users', 1),\n",
       " ('choose', 'never', 1),\n",
       " ('choose', 'words', 1),\n",
       " ('choose', 'randomly', 1),\n",
       " ('choose', 'essentially', 0),\n",
       " ('choose', '.', 0),\n",
       " ('choose', ',', 0),\n",
       " ('choose', '.', 0),\n",
       " ('words', 'never', 1),\n",
       " ('words', 'choose', 1),\n",
       " ('words', 'randomly', 1),\n",
       " ('words', ',', 1),\n",
       " ('words', 'is', 0),\n",
       " ('words', 'non-random', 0),\n",
       " ('words', '.', 0),\n",
       " ('words', 'language', 0),\n",
       " ('randomly', 'choose', 1),\n",
       " ('randomly', 'words', 1),\n",
       " ('randomly', ',', 1),\n",
       " ('randomly', 'and', 1),\n",
       " ('randomly', 'never', 0),\n",
       " ('randomly', 'never', 0),\n",
       " ('randomly', '.', 0),\n",
       " ('randomly', 'users', 0),\n",
       " (',', 'words', 1),\n",
       " (',', 'randomly', 1),\n",
       " (',', 'and', 1),\n",
       " (',', 'language', 1),\n",
       " (',', 'essentially', 0),\n",
       " (',', 'is', 0),\n",
       " (',', 'choose', 0),\n",
       " (',', 'never', 0),\n",
       " ('and', 'randomly', 1),\n",
       " ('and', ',', 1),\n",
       " ('and', 'language', 1),\n",
       " ('and', 'is', 1),\n",
       " ('and', 'language', 0),\n",
       " ('and', 'essentially', 0),\n",
       " ('and', 'never', 0),\n",
       " ('and', 'choose', 0),\n",
       " ('language', ',', 1),\n",
       " ('language', 'and', 1),\n",
       " ('language', 'is', 1),\n",
       " ('language', 'essentially', 1),\n",
       " ('language', 'language', 0),\n",
       " ('language', 'language', 0),\n",
       " ('language', 'users', 0),\n",
       " ('language', 'words', 0),\n",
       " ('is', 'and', 1),\n",
       " ('is', 'language', 1),\n",
       " ('is', 'essentially', 1),\n",
       " ('is', 'non-random', 1),\n",
       " ('is', '.', 0),\n",
       " ('is', '.', 0),\n",
       " ('is', '.', 0),\n",
       " ('is', 'words', 0),\n",
       " ('essentially', 'language', 1),\n",
       " ('essentially', 'is', 1),\n",
       " ('essentially', 'non-random', 1),\n",
       " ('essentially', '.', 1),\n",
       " ('essentially', 'randomly', 0),\n",
       " ('essentially', 'never', 0),\n",
       " ('essentially', 'choose', 0),\n",
       " ('essentially', 'choose', 0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(skipgram_iterator(sent0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MR1oCgnCHAMM"
   },
   "source": [
    "## Cut-away: What is `partial`?\n",
    "\n",
    "The [`functools.partial`](https://docs.python.org/3.7/library/functools.html#functools.partial) function in Python is a mechanism to overload a function with preset arguments. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o37H0cxxHAMN",
    "outputId": "d4e2f38e-e33f-4320-bc7c-d05cf9589b79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Generates bigrams\n",
    "list(ngrams('this is a sentence'.split(), n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFdDjEZvHAMd"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# You can create a new function that \"preset\" the `n` argument, e.g.\n",
    "bigrams = partial(ngrams, n=2)\n",
    "trigrams = partial(ngrams, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5gDUuojHAMg",
    "outputId": "c7bcb042-ee0d-4c53-85ec-c555f46001e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is', 'a'), ('is', 'a', 'sentence')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigrams('this is a sentence'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DnZTSkGIHAMn",
    "outputId": "9e3c3ea0-74d7-4ae6-906f-6ef0448ad93d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams('this is a sentence'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7v5X8GlHAMw"
   },
   "source": [
    "<a id=\"section-3-1-3\"></a>\n",
    "\n",
    "## 3.1.3 Word2Vec Dataset\n",
    "\n",
    "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
    "\n",
    "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UaPLt9WrHAMx"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "\n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized.\n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx\n",
    "        return self.vocab.doc2idx(tokens)\n",
    "\n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "\n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x':window, 'y':target}   # X = window ; Y = target. \n",
    "        \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            target = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x':(target, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                yield {'x': (target, random.choice(leftovers)), 'y':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXE9ukTcHAM2"
   },
   "source": [
    "<a id=\"section-3-1-3-hint\"></a>\n",
    "## Hints for the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymXcYy25HAM5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
    "##hint_word2vec_dataset()\n",
    "\n",
    "# Option 2: \"I give up just, run the code for me\" \n",
    "# Uncomment the next two lines, if you really gave up... \n",
    "##full_code_word2vec_dataset()\n",
    "##from tsundoku.word2vec import Word2VecText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXFpDkcXHAM9"
   },
   "source": [
    "<a id=\"section-3-1-4-hint\"></a>\n",
    "\n",
    "## 3.1.4. Train a CBOW model\n",
    "\n",
    "### Lets Get Some Data\n",
    "\n",
    "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEYnPscyHANA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io #codecs\n",
    "\n",
    "\n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "    text = requests.get(url).content.decode('utf8')\n",
    "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOFKsFVZHANE",
    "outputId": "39bed45c-d098-4726-a5a6-0bbb4281d1e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Language is never, ever, ever, random\n",
      "\n",
      "                                      \n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfarUjiGHANJ",
    "outputId": "4c946f67-f4df-40f8-fc6a-6d6c9539127e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check, lets take a look at the data.\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRmEPBJbHANP"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KJ4NHacHANS",
    "outputId": "63500498-f0f0-40f3-8da2-86027094c125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n",
      "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91m______\u001b[0m linguistic questions\n",
      "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91m______\u001b[0m questions concern\n",
      "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91m______\u001b[0m concern the\n",
      "\u001b[92mconcern\u001b[0m \t linguistic questions \u001b[91m______\u001b[0m the dis-\n",
      "\u001b[92mthe\u001b[0m \t\t questions concern \u001b[91m______\u001b[0m dis- tinction\n",
      "\u001b[92mdis-\u001b[0m \t\t concern the \u001b[91m______\u001b[0m tinction between\n",
      "\u001b[92mtinction\u001b[0m \t the dis- \u001b[91m______\u001b[0m between a\n",
      "\u001b[92mbetween\u001b[0m \t dis- tinction \u001b[91m______\u001b[0m a and\n",
      "\u001b[92ma\u001b[0m \t\t tinction between \u001b[91m______\u001b[0m and m.\n",
      "\u001b[92mand\u001b[0m \t\t between a \u001b[91m______\u001b[0m m. a\n",
      "\u001b[92mm.\u001b[0m \t\t a and \u001b[91m______\u001b[0m a linguistic\n",
      "\u001b[92ma\u001b[0m \t\t and m. \u001b[91m______\u001b[0m linguistic account\n",
      "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91m______\u001b[0m account of\n",
      "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91m______\u001b[0m of a\n",
      "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91m______\u001b[0m a phenomenon\n",
      "\u001b[92ma\u001b[0m \t\t account of \u001b[91m______\u001b[0m phenomenon gen-\n",
      "\u001b[92mphenomenon\u001b[0m \t of a \u001b[91m______\u001b[0m gen- erally\n",
      "\u001b[92mgen-\u001b[0m \t\t a phenomenon \u001b[91m______\u001b[0m erally gives\n",
      "\u001b[92merally\u001b[0m \t\t phenomenon gen- \u001b[91m______\u001b[0m gives us\n",
      "\u001b[92mgives\u001b[0m \t\t gen- erally \u001b[91m______\u001b[0m us reason\n",
      "\u001b[92mus\u001b[0m \t\t erally gives \u001b[91m______\u001b[0m reason to\n",
      "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m______\u001b[0m to view\n",
      "\u001b[92mto\u001b[0m \t\t us reason \u001b[91m______\u001b[0m view the\n",
      "\u001b[92mview\u001b[0m \t\t reason to \u001b[91m______\u001b[0m the relation\n",
      "\u001b[92mthe\u001b[0m \t\t to view \u001b[91m______\u001b[0m relation between\n",
      "\u001b[92mrelation\u001b[0m \t view the \u001b[91m______\u001b[0m between ,\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91m______\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91m______\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91m______\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91m______\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
      "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91m______\u001b[0m its semantics\n",
      "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91m______\u001b[0m semantics ,\n",
      "\u001b[92msemantics\u001b[0m \t and its \u001b[91m______\u001b[0m , as\n",
      "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91m______\u001b[0m as motivated\n",
      "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91m______\u001b[0m motivated rather\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
    "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
    "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
    "    target = vocab.get(int(y), '<unk>')\n",
    "\n",
    "    if not prediction:\n",
    "        predicted_word = '______'\n",
    "    else:\n",
    "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
    "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
    "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
    "    \n",
    "\n",
    "sent_idx = 10\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
    "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
    "for w2v_io in w2v_dataset[sent_idx]:\n",
    "    context, target = w2v_io['x'], w2v_io['y']\n",
    "    context, target = tensor(context).to(device), tensor(target).to(device)\n",
    "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCTIgQLwHAOM"
   },
   "source": [
    "<a id=\"section-3-1-4-cbow-model\"></a>\n",
    "\n",
    "## The CBOW Model\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"500\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVhbh2OoHAON"
   },
   "source": [
    "(Image from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bE4_o2uHAOO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0XM-9CoHAOQ"
   },
   "source": [
    "## Lets take a closer look from the inputs to the first `nn.Linear`\n",
    "\n",
    "Cos after it reach the first `nn.Linear` it's just the same as our multi-layered perceptron example =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VWtD5VoxHAOR",
    "outputId": "e602d428-841e-4caf-c8c1-96139cb58fde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': [10, 8, 0, 7], 'y': 11},\n",
       " {'x': [8, 11, 7, 0], 'y': 0},\n",
       " {'x': [11, 0, 0, 7], 'y': 7},\n",
       " {'x': [0, 7, 7, 0], 'y': 0},\n",
       " {'x': [7, 0, 0, 13], 'y': 7},\n",
       " {'x': [0, 7, 13, 3], 'y': 0},\n",
       " {'x': [7, 0, 3, 9], 'y': 13},\n",
       " {'x': [0, 13, 9, 2], 'y': 3},\n",
       " {'x': [13, 3, 2, 10], 'y': 9},\n",
       " {'x': [3, 9, 10, 15], 'y': 2},\n",
       " {'x': [9, 2, 15, 11], 'y': 10},\n",
       " {'x': [2, 10, 11, 5], 'y': 15},\n",
       " {'x': [10, 15, 5, 16], 'y': 11},\n",
       " {'x': [15, 11, 16, 14], 'y': 5},\n",
       " {'x': [11, 5, 14, 0], 'y': 16},\n",
       " {'x': [5, 16, 0, 4], 'y': 14},\n",
       " {'x': [16, 14, 4, 10], 'y': 0},\n",
       " {'x': [14, 0, 10, 8], 'y': 4},\n",
       " {'x': [0, 4, 8, 6], 'y': 10},\n",
       " {'x': [4, 10, 6, 12], 'y': 8},\n",
       " {'x': [10, 8, 12, 1], 'y': 6}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YrihyEczHAOa",
    "outputId": "3e24e240-1b66-443a-fa8b-be608b5b7e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  8,  0,  7])\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "# Lets take a look at the first output.\n",
    "\n",
    "\n",
    "x, y = w2v_dataset[0][0]['x'],  w2v_dataset[0][0]['y']\n",
    "\n",
    "x = tensor(x)\n",
    "#y = autograd.Variable(tensor(y, dtype=torch.long))\n",
    "y = tensor(y, dtype=torch.long)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSQ1tZxaHAOd",
    "outputId": "5829ea69-2a46-4cf6-e542-bd150de0f2c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-5.8898e-01, -2.0540e-01,  2.9055e-01,  1.9336e-01, -1.0842e+00],\n",
       "                      [-1.0371e+00, -2.1308e+00,  9.7002e-01, -2.5638e+00, -4.5461e-01],\n",
       "                      [-4.4496e-04, -1.0276e+00, -7.2960e-01,  1.5938e+00, -2.2959e-01],\n",
       "                      ...,\n",
       "                      [-2.3368e-01,  3.3243e-01,  1.0829e+00, -1.1885e+00, -6.6717e-01],\n",
       "                      [ 2.4669e-01, -1.0625e-01,  4.2414e-02, -2.6352e-01,  5.9180e-01],\n",
       "                      [-1.7882e+00, -2.0268e-01,  7.0522e-02,  6.5959e-01, -1.4453e+00]]))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_size = 5\n",
    "emb = nn.Embedding(len(w2v_dataset.vocab), embd_size)\n",
    "emb.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMSpy8GNHAOf",
    "outputId": "83be6b9c-7fc2-4ef8-e420-2e680fb88dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1388, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8898e-01, -2.0540e-01,  2.9055e-01,  1.9336e-01, -1.0842e+00],\n",
       "        [-1.0371e+00, -2.1308e+00,  9.7002e-01, -2.5638e+00, -4.5461e-01],\n",
       "        [-4.4496e-04, -1.0276e+00, -7.2960e-01,  1.5938e+00, -2.2959e-01],\n",
       "        ...,\n",
       "        [-2.3368e-01,  3.3243e-01,  1.0829e+00, -1.1885e+00, -6.6717e-01],\n",
       "        [ 2.4669e-01, -1.0625e-01,  4.2414e-02, -2.6352e-01,  5.9180e-01],\n",
       "        [-1.7882e+00, -2.0268e-01,  7.0522e-02,  6.5959e-01, -1.4453e+00]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emb.state_dict()['weight'].shape)\n",
    "emb.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  8,  0,  7])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_5qYpO8HAOh",
    "outputId": "dfb5fe3d-7f02-407b-ee86-50c86179917c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "tensor([[-0.2246, -0.5290, -1.1029,  0.4685,  1.4300],\n",
      "        [ 0.7285, -0.9114, -1.1336,  0.0280, -2.2049],\n",
      "        [-0.5890, -0.2054,  0.2905,  0.1934, -1.0842],\n",
      "        [ 1.2866,  0.3870, -1.9086, -0.0425, -0.5661]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(emb(x).shape)\n",
    "print(emb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suT4Nm24HAOj",
    "outputId": "9e348f3b-fc50-4bba-981a-86448bd21775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2246, -0.5290, -1.1029,  0.4685,  1.4300,  0.7285, -0.9114, -1.1336,\n",
       "          0.0280, -2.2049, -0.5890, -0.2054,  0.2905,  0.1934, -1.0842,  1.2866,\n",
       "          0.3870, -1.9086, -0.0425, -0.5661]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emb(x).view(1, -1).shape)\n",
    "emb(x).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Trdx-g3SHAOl",
    "outputId": "c4720d0f-0acc-4e8c-ce73-47f84cf1f564",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20])\n",
      "OrderedDict([('weight', tensor([[-0.1288, -0.0355, -0.1723,  ..., -0.0951, -0.0751, -0.1592],\n",
      "        [-0.0866,  0.1689,  0.2230,  ..., -0.0883, -0.2132, -0.1075],\n",
      "        [ 0.1124, -0.1691,  0.0147,  ...,  0.1674,  0.0595, -0.0788],\n",
      "        ...,\n",
      "        [-0.1023,  0.0762, -0.0567,  ..., -0.1648,  0.1905,  0.0958],\n",
      "        [ 0.1656, -0.0004, -0.1277,  ...,  0.0986, -0.1910, -0.0423],\n",
      "        [-0.1800,  0.1981, -0.1839,  ..., -0.0899, -0.0744,  0.1923]])), ('bias', tensor([-0.2151, -0.0034, -0.0790, -0.1838, -0.1932, -0.1374, -0.1299, -0.1450,\n",
      "         0.1796,  0.1060, -0.0734,  0.0206, -0.1693, -0.0543, -0.1511, -0.1363,\n",
      "         0.0927,  0.1495,  0.0113, -0.2060,  0.2118,  0.2151, -0.1734, -0.2082,\n",
      "        -0.0528,  0.1025, -0.1669, -0.2165, -0.1839, -0.0355, -0.0461,  0.1883,\n",
      "         0.1944, -0.0562, -0.1225,  0.0427, -0.2002,  0.0016, -0.1619, -0.1814,\n",
      "        -0.1062,  0.2188,  0.1383,  0.1798, -0.0454, -0.0545, -0.1184,  0.0300,\n",
      "         0.2026, -0.1039,  0.1661, -0.2174,  0.2120, -0.1831,  0.1304,  0.0369,\n",
      "        -0.0909, -0.1432,  0.1915,  0.1587, -0.2134, -0.1771,  0.0657, -0.1230,\n",
      "        -0.0785,  0.0538, -0.0169,  0.1560, -0.2043,  0.1390, -0.2147, -0.1455,\n",
      "        -0.0278, -0.0368, -0.0776,  0.1150,  0.2150, -0.0192, -0.1162, -0.2113,\n",
      "         0.0759,  0.0242,  0.1542,  0.0450,  0.1617, -0.0642,  0.1981, -0.1946,\n",
      "         0.0597, -0.0093,  0.1727, -0.1221,  0.0630,  0.1957, -0.1805,  0.1379,\n",
      "         0.0326,  0.0700,  0.0035,  0.0632]))])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "lin1 = nn.Linear(len(x)*embd_size, hidden_size)\n",
    "print(lin1.state_dict()['weight'].shape)\n",
    "print(lin1.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pekv5vr3HAOn",
    "outputId": "9120d23d-3dbf-4690-b0e0-dc246f160a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20])\n",
      "tensor([[-0.1288, -0.0355, -0.1723,  ..., -0.0951, -0.0751, -0.1592],\n",
      "        [-0.0866,  0.1689,  0.2230,  ..., -0.0883, -0.2132, -0.1075],\n",
      "        [ 0.1124, -0.1691,  0.0147,  ...,  0.1674,  0.0595, -0.0788],\n",
      "        ...,\n",
      "        [-0.1023,  0.0762, -0.0567,  ..., -0.1648,  0.1905,  0.0958],\n",
      "        [ 0.1656, -0.0004, -0.1277,  ...,  0.0986, -0.1910, -0.0423],\n",
      "        [-0.1800,  0.1981, -0.1839,  ..., -0.0899, -0.0744,  0.1923]])\n"
     ]
    }
   ],
   "source": [
    "print(lin1.state_dict()['weight'].shape)\n",
    "print(lin1.state_dict()['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8PYFtEKHAOn",
    "outputId": "7a53ced5-d039-40c1-f35e-c48aac1a0ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2185, -0.0496, -0.1866, -0.5017,  0.4579,  0.1790,  0.0327, -0.4299,\n",
       "          0.3196, -0.1169, -0.3991, -0.7709,  0.1475, -0.4278, -0.9297,  0.0439,\n",
       "          0.1469,  0.6108,  0.0887, -0.7961,  0.3342, -0.2362,  0.1492,  0.3411,\n",
       "         -0.0341, -0.5492,  0.0894,  0.0250, -0.3739,  1.3482,  0.1073,  0.4159,\n",
       "         -0.5619,  0.3285, -0.6849,  0.1828,  0.1920,  0.7376, -0.6074,  0.1332,\n",
       "          0.3172, -0.1025, -0.0837, -0.1131,  0.3069,  0.6064,  0.2612, -0.2116,\n",
       "          0.5163,  0.1291,  0.0670, -0.1200,  1.9037,  0.5940,  0.9955,  0.2376,\n",
       "         -0.4260,  0.2031, -0.0780,  0.5926, -0.2960, -0.9214, -0.7533, -0.6357,\n",
       "          0.8338, -0.5698,  0.5175,  0.1766,  0.0695, -0.4947,  0.0817, -0.3715,\n",
       "          0.7642,  0.0060, -0.5231, -0.2313,  0.4208, -0.2985,  0.4007, -0.4862,\n",
       "         -0.3533, -0.1134, -0.5583,  0.4015,  0.1669, -0.5890,  0.8274,  0.6932,\n",
       "          1.4397, -0.5085, -0.4196, -0.4862,  1.0390, -0.9562, -0.1966,  0.1001,\n",
       "         -0.2638,  0.5256,  0.0144,  0.7556]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lin1(emb(x).view(1, -1)).shape)\n",
    "lin1(emb(x).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnMag-0tHAOp",
    "outputId": "559bea7c-34e1-42d2-b1b3-7457e00b7f7e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2185, 0.0000, 0.0000, 0.0000, 0.4579, 0.1790, 0.0327, 0.0000, 0.3196,\n",
       "         0.0000, 0.0000, 0.0000, 0.1475, 0.0000, 0.0000, 0.0439, 0.1469, 0.6108,\n",
       "         0.0887, 0.0000, 0.3342, 0.0000, 0.1492, 0.3411, 0.0000, 0.0000, 0.0894,\n",
       "         0.0250, 0.0000, 1.3482, 0.1073, 0.4159, 0.0000, 0.3285, 0.0000, 0.1828,\n",
       "         0.1920, 0.7376, 0.0000, 0.1332, 0.3172, 0.0000, 0.0000, 0.0000, 0.3069,\n",
       "         0.6064, 0.2612, 0.0000, 0.5163, 0.1291, 0.0670, 0.0000, 1.9037, 0.5940,\n",
       "         0.9955, 0.2376, 0.0000, 0.2031, 0.0000, 0.5926, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.8338, 0.0000, 0.5175, 0.1766, 0.0695, 0.0000, 0.0817, 0.0000,\n",
       "         0.7642, 0.0060, 0.0000, 0.0000, 0.4208, 0.0000, 0.4007, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.4015, 0.1669, 0.0000, 0.8274, 0.6932, 1.4397, 0.0000,\n",
       "         0.0000, 0.0000, 1.0390, 0.0000, 0.0000, 0.1001, 0.0000, 0.5256, 0.0144,\n",
       "         0.7556]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "print(relu(lin1(emb(x).view(1, -1))).shape)\n",
    "relu(lin1(emb(x).view(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVwej9y_HAOr",
    "outputId": "b2f5a8fc-bf76-42c7-8fdd-4d1ed4db4e24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1388, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0606, -0.0397, -0.0610,  ...,  0.0172,  0.0905,  0.0366],\n",
       "        [-0.0327,  0.0837, -0.0138,  ...,  0.0322, -0.0874,  0.0039],\n",
       "        [ 0.0980, -0.0244, -0.0406,  ...,  0.0469,  0.0429,  0.0531],\n",
       "        ...,\n",
       "        [ 0.0142,  0.0087,  0.0808,  ...,  0.0624, -0.0208,  0.0690],\n",
       "        [ 0.0258, -0.0939, -0.0442,  ..., -0.0825,  0.0853,  0.0938],\n",
       "        [-0.0205,  0.0148,  0.0427,  ...,  0.0383, -0.0394,  0.0718]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin2 = nn.Linear(hidden_size, len(w2v_dataset.vocab))\n",
    "print(lin2.state_dict()['weight'].shape)\n",
    "lin2.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "asgRwXehHAOs",
    "outputId": "dd545005-24ae-49d6-922a-c63393c4472f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1388])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0325, -0.2789,  0.5067,  ...,  0.2830,  0.0342,  0.0100]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_x = relu(lin1(emb(x).view(1, -1)))\n",
    "print(lin2(h_x).shape)\n",
    "lin2(h_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GJLGwdkHAOv",
    "outputId": "22a0b70d-977e-47f8-98ca-18f90f248b0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-7.221717834472656,\n",
       "  -7.53314733505249,\n",
       "  -6.7474684715271,\n",
       "  -7.314044952392578,\n",
       "  -7.249974250793457,\n",
       "  -7.16320276260376,\n",
       "  -6.923361778259277,\n",
       "  -7.451221466064453,\n",
       "  -7.36537504196167,\n",
       "  -7.3676252365112305,\n",
       "  -7.095677852630615,\n",
       "  -7.642803192138672,\n",
       "  -7.254631042480469,\n",
       "  -6.99247932434082,\n",
       "  -7.012055397033691,\n",
       "  -7.489522457122803,\n",
       "  -7.478002548217773,\n",
       "  -7.487682342529297,\n",
       "  -7.0211944580078125,\n",
       "  -7.2627997398376465,\n",
       "  -7.04066276550293,\n",
       "  -7.043307304382324,\n",
       "  -7.529435157775879,\n",
       "  -7.466611862182617,\n",
       "  -7.3697333335876465,\n",
       "  -6.965380668640137,\n",
       "  -7.351118564605713,\n",
       "  -7.030649662017822,\n",
       "  -7.175492286682129,\n",
       "  -6.681637287139893,\n",
       "  -7.510779857635498,\n",
       "  -7.216093063354492,\n",
       "  -7.360151290893555,\n",
       "  -7.132218360900879,\n",
       "  -7.5915045738220215,\n",
       "  -7.363449573516846,\n",
       "  -7.1642746925354,\n",
       "  -7.413661956787109,\n",
       "  -6.806906700134277,\n",
       "  -7.147624492645264,\n",
       "  -6.957555770874023,\n",
       "  -7.099673748016357,\n",
       "  -7.490873336791992,\n",
       "  -7.026181697845459,\n",
       "  -7.198056697845459,\n",
       "  -7.3237409591674805,\n",
       "  -7.229017734527588,\n",
       "  -7.33970308303833,\n",
       "  -7.021090984344482,\n",
       "  -6.839694023132324,\n",
       "  -7.037384986877441,\n",
       "  -7.208944320678711,\n",
       "  -7.094302654266357,\n",
       "  -7.381300449371338,\n",
       "  -6.574077129364014,\n",
       "  -7.431546211242676,\n",
       "  -7.429415702819824,\n",
       "  -7.357115268707275,\n",
       "  -7.747076034545898,\n",
       "  -6.958355903625488,\n",
       "  -7.434196472167969,\n",
       "  -6.938600540161133,\n",
       "  -7.557370185852051,\n",
       "  -7.292884826660156,\n",
       "  -7.527582168579102,\n",
       "  -6.953342437744141,\n",
       "  -7.260960578918457,\n",
       "  -7.374963283538818,\n",
       "  -7.921099662780762,\n",
       "  -7.40766716003418,\n",
       "  -7.178297996520996,\n",
       "  -7.3668389320373535,\n",
       "  -7.406957626342773,\n",
       "  -7.223256587982178,\n",
       "  -7.743711948394775,\n",
       "  -7.502164840698242,\n",
       "  -7.130455493927002,\n",
       "  -7.276754856109619,\n",
       "  -7.305844783782959,\n",
       "  -7.033456325531006,\n",
       "  -7.101699352264404,\n",
       "  -7.144909381866455,\n",
       "  -7.500372886657715,\n",
       "  -6.920791149139404,\n",
       "  -6.99962854385376,\n",
       "  -7.176354885101318,\n",
       "  -7.360880374908447,\n",
       "  -7.408695220947266,\n",
       "  -7.29580020904541,\n",
       "  -7.581492900848389,\n",
       "  -7.078063011169434,\n",
       "  -7.429959297180176,\n",
       "  -6.758188247680664,\n",
       "  -7.378240585327148,\n",
       "  -7.026021480560303,\n",
       "  -6.793659687042236,\n",
       "  -7.3666157722473145,\n",
       "  -7.631613254547119,\n",
       "  -7.003971576690674,\n",
       "  -7.189113140106201,\n",
       "  -7.370584487915039,\n",
       "  -6.687582969665527,\n",
       "  -6.667448043823242,\n",
       "  -6.849948883056641,\n",
       "  -7.167782783508301,\n",
       "  -7.434806823730469,\n",
       "  -7.030579090118408,\n",
       "  -6.914714336395264,\n",
       "  -7.48103666305542,\n",
       "  -7.204628944396973,\n",
       "  -7.5106096267700195,\n",
       "  -7.513815879821777,\n",
       "  -7.590305328369141,\n",
       "  -7.550200462341309,\n",
       "  -7.321963787078857,\n",
       "  -7.51542329788208,\n",
       "  -7.015151023864746,\n",
       "  -7.314125061035156,\n",
       "  -7.5854573249816895,\n",
       "  -7.683818817138672,\n",
       "  -7.297431945800781,\n",
       "  -7.262411117553711,\n",
       "  -7.4195404052734375,\n",
       "  -7.379909515380859,\n",
       "  -7.394016265869141,\n",
       "  -7.637887954711914,\n",
       "  -7.24298095703125,\n",
       "  -7.157647132873535,\n",
       "  -7.276615142822266,\n",
       "  -7.1199750900268555,\n",
       "  -7.198707580566406,\n",
       "  -7.113290786743164,\n",
       "  -7.410717964172363,\n",
       "  -7.011724472045898,\n",
       "  -7.031436443328857,\n",
       "  -7.455336570739746,\n",
       "  -7.377182960510254,\n",
       "  -7.488311767578125,\n",
       "  -7.224261283874512,\n",
       "  -7.352403163909912,\n",
       "  -7.253929138183594,\n",
       "  -6.919299125671387,\n",
       "  -7.64703369140625,\n",
       "  -6.855388164520264,\n",
       "  -7.142796993255615,\n",
       "  -7.066834449768066,\n",
       "  -7.4740777015686035,\n",
       "  -6.505115985870361,\n",
       "  -6.917304992675781,\n",
       "  -7.594794273376465,\n",
       "  -7.16546106338501,\n",
       "  -7.02669620513916,\n",
       "  -7.117742538452148,\n",
       "  -7.409332275390625,\n",
       "  -6.732608795166016,\n",
       "  -7.237565994262695,\n",
       "  -7.331297397613525,\n",
       "  -7.1041717529296875,\n",
       "  -7.074731826782227,\n",
       "  -7.155708312988281,\n",
       "  -7.513238906860352,\n",
       "  -7.235239028930664,\n",
       "  -7.157317638397217,\n",
       "  -7.233892440795898,\n",
       "  -7.186144828796387,\n",
       "  -7.644933700561523,\n",
       "  -7.480815887451172,\n",
       "  -7.140183448791504,\n",
       "  -7.4884538650512695,\n",
       "  -7.518817901611328,\n",
       "  -7.058204174041748,\n",
       "  -7.060018062591553,\n",
       "  -7.419284820556641,\n",
       "  -7.3284525871276855,\n",
       "  -7.330336093902588,\n",
       "  -7.130813121795654,\n",
       "  -7.087424278259277,\n",
       "  -7.367138862609863,\n",
       "  -7.354429244995117,\n",
       "  -7.167945384979248,\n",
       "  -7.625213623046875,\n",
       "  -7.180617332458496,\n",
       "  -7.073854446411133,\n",
       "  -7.207615375518799,\n",
       "  -6.888605117797852,\n",
       "  -7.9095048904418945,\n",
       "  -7.232958793640137,\n",
       "  -7.272602558135986,\n",
       "  -6.774403095245361,\n",
       "  -7.297952175140381,\n",
       "  -7.029579162597656,\n",
       "  -7.153996467590332,\n",
       "  -7.203371524810791,\n",
       "  -6.749183177947998,\n",
       "  -7.2117815017700195,\n",
       "  -7.187165260314941,\n",
       "  -6.959379196166992,\n",
       "  -7.736281394958496,\n",
       "  -7.29328727722168,\n",
       "  -7.662507057189941,\n",
       "  -7.733719825744629,\n",
       "  -7.3491387367248535,\n",
       "  -6.916773319244385,\n",
       "  -7.4004974365234375,\n",
       "  -6.823277473449707,\n",
       "  -7.025692939758301,\n",
       "  -7.0201215744018555,\n",
       "  -6.8976593017578125,\n",
       "  -7.061929702758789,\n",
       "  -7.059905052185059,\n",
       "  -6.923762321472168,\n",
       "  -7.2292160987854,\n",
       "  -7.278704643249512,\n",
       "  -6.991617202758789,\n",
       "  -7.333342552185059,\n",
       "  -7.178731441497803,\n",
       "  -7.6695075035095215,\n",
       "  -7.74287223815918,\n",
       "  -7.211413383483887,\n",
       "  -7.141373634338379,\n",
       "  -7.853737831115723,\n",
       "  -6.861963748931885,\n",
       "  -7.501158237457275,\n",
       "  -7.105318069458008,\n",
       "  -7.176870346069336,\n",
       "  -7.326045513153076,\n",
       "  -7.717841148376465,\n",
       "  -7.517961025238037,\n",
       "  -7.538115978240967,\n",
       "  -6.820570468902588,\n",
       "  -7.659729957580566,\n",
       "  -6.77294397354126,\n",
       "  -7.649231910705566,\n",
       "  -7.244340896606445,\n",
       "  -7.3071136474609375,\n",
       "  -7.373262405395508,\n",
       "  -7.278897285461426,\n",
       "  -7.104410648345947,\n",
       "  -7.866848945617676,\n",
       "  -7.771798133850098,\n",
       "  -7.241407871246338,\n",
       "  -7.211850166320801,\n",
       "  -7.733242511749268,\n",
       "  -7.1798858642578125,\n",
       "  -7.571267127990723,\n",
       "  -8.034439086914062,\n",
       "  -7.583970069885254,\n",
       "  -7.143028259277344,\n",
       "  -7.520860195159912,\n",
       "  -7.19691276550293,\n",
       "  -7.482710361480713,\n",
       "  -7.07948112487793,\n",
       "  -7.469593048095703,\n",
       "  -7.092052459716797,\n",
       "  -7.273797988891602,\n",
       "  -7.359000205993652,\n",
       "  -6.999208450317383,\n",
       "  -7.059813976287842,\n",
       "  -7.548218727111816,\n",
       "  -6.892848014831543,\n",
       "  -7.260315418243408,\n",
       "  -7.379019737243652,\n",
       "  -7.120248794555664,\n",
       "  -7.183216094970703,\n",
       "  -7.105192184448242,\n",
       "  -7.336616516113281,\n",
       "  -6.562376976013184,\n",
       "  -7.92991304397583,\n",
       "  -7.081076145172119,\n",
       "  -7.121894836425781,\n",
       "  -7.533529758453369,\n",
       "  -7.124813556671143,\n",
       "  -7.6864333152771,\n",
       "  -7.1304216384887695,\n",
       "  -7.324635982513428,\n",
       "  -7.372840881347656,\n",
       "  -7.538376331329346,\n",
       "  -7.234733581542969,\n",
       "  -7.365303039550781,\n",
       "  -6.94026517868042,\n",
       "  -7.5940704345703125,\n",
       "  -7.0980143547058105,\n",
       "  -7.722995281219482,\n",
       "  -7.602541446685791,\n",
       "  -7.428195476531982,\n",
       "  -7.432166576385498,\n",
       "  -7.009383201599121,\n",
       "  -7.088448524475098,\n",
       "  -7.222404956817627,\n",
       "  -7.358040809631348,\n",
       "  -7.2952470779418945,\n",
       "  -7.448058128356934,\n",
       "  -7.208365440368652,\n",
       "  -7.324885845184326,\n",
       "  -6.883885860443115,\n",
       "  -6.866087436676025,\n",
       "  -6.916475772857666,\n",
       "  -7.4943084716796875,\n",
       "  -7.698406219482422,\n",
       "  -6.801068305969238,\n",
       "  -7.32155704498291,\n",
       "  -7.28311014175415,\n",
       "  -6.77326774597168,\n",
       "  -7.357603073120117,\n",
       "  -7.3375701904296875,\n",
       "  -7.211195468902588,\n",
       "  -7.501868724822998,\n",
       "  -7.400957107543945,\n",
       "  -7.459656715393066,\n",
       "  -6.940827369689941,\n",
       "  -7.426206588745117,\n",
       "  -7.310019016265869,\n",
       "  -7.487460136413574,\n",
       "  -7.365739822387695,\n",
       "  -7.047320365905762,\n",
       "  -7.347037315368652,\n",
       "  -7.07396125793457,\n",
       "  -7.609187602996826,\n",
       "  -7.069035053253174,\n",
       "  -7.082282543182373,\n",
       "  -7.200909614562988,\n",
       "  -7.387637138366699,\n",
       "  -7.306534767150879,\n",
       "  -7.424386501312256,\n",
       "  -6.903275966644287,\n",
       "  -6.998372554779053,\n",
       "  -7.19229793548584,\n",
       "  -7.285061836242676,\n",
       "  -7.106836318969727,\n",
       "  -7.136682033538818,\n",
       "  -7.083719730377197,\n",
       "  -7.640603542327881,\n",
       "  -7.318314075469971,\n",
       "  -7.370642185211182,\n",
       "  -7.4606404304504395,\n",
       "  -7.344852924346924,\n",
       "  -7.056897163391113,\n",
       "  -7.467764377593994,\n",
       "  -7.148303031921387,\n",
       "  -7.270544052124023,\n",
       "  -7.079771995544434,\n",
       "  -7.185034275054932,\n",
       "  -7.159749507904053,\n",
       "  -7.0676727294921875,\n",
       "  -7.095918655395508,\n",
       "  -7.088251113891602,\n",
       "  -6.954614639282227,\n",
       "  -7.289216041564941,\n",
       "  -7.762306213378906,\n",
       "  -7.226046085357666,\n",
       "  -7.657061576843262,\n",
       "  -7.771238327026367,\n",
       "  -7.619790077209473,\n",
       "  -7.229001045227051,\n",
       "  -7.145942687988281,\n",
       "  -6.7828240394592285,\n",
       "  -7.520084857940674,\n",
       "  -7.778029441833496,\n",
       "  -7.061226844787598,\n",
       "  -6.9580864906311035,\n",
       "  -6.776764869689941,\n",
       "  -7.268063545227051,\n",
       "  -6.900124549865723,\n",
       "  -7.630980014801025,\n",
       "  -7.1224365234375,\n",
       "  -7.351249694824219,\n",
       "  -7.483229637145996,\n",
       "  -7.454099655151367,\n",
       "  -7.039983749389648,\n",
       "  -7.500222682952881,\n",
       "  -7.082394599914551,\n",
       "  -7.0105061531066895,\n",
       "  -7.244571685791016,\n",
       "  -7.247378826141357,\n",
       "  -7.171417236328125,\n",
       "  -7.312107086181641,\n",
       "  -7.330967426300049,\n",
       "  -7.289364814758301,\n",
       "  -7.581753253936768,\n",
       "  -7.267581462860107,\n",
       "  -7.048942565917969,\n",
       "  -7.276209831237793,\n",
       "  -7.281101226806641,\n",
       "  -7.18779182434082,\n",
       "  -7.191030502319336,\n",
       "  -7.138637542724609,\n",
       "  -7.174879550933838,\n",
       "  -7.067686080932617,\n",
       "  -7.289472579956055,\n",
       "  -7.295705795288086,\n",
       "  -7.136231899261475,\n",
       "  -6.618697166442871,\n",
       "  -6.903059482574463,\n",
       "  -7.421512603759766,\n",
       "  -7.4579386711120605,\n",
       "  -7.192225933074951,\n",
       "  -7.182563781738281,\n",
       "  -7.024908542633057,\n",
       "  -7.274827480316162,\n",
       "  -7.434971332550049,\n",
       "  -6.912428379058838,\n",
       "  -7.10395622253418,\n",
       "  -7.216513156890869,\n",
       "  -7.1763386726379395,\n",
       "  -6.980027198791504,\n",
       "  -7.266312599182129,\n",
       "  -7.402410507202148,\n",
       "  -7.191573143005371,\n",
       "  -7.261918067932129,\n",
       "  -6.866729259490967,\n",
       "  -7.467292785644531,\n",
       "  -6.986081600189209,\n",
       "  -7.625190734863281,\n",
       "  -6.910805702209473,\n",
       "  -7.299818992614746,\n",
       "  -7.0843915939331055,\n",
       "  -6.8991780281066895,\n",
       "  -7.049831867218018,\n",
       "  -7.66523551940918,\n",
       "  -7.346725940704346,\n",
       "  -7.169081211090088,\n",
       "  -7.288435935974121,\n",
       "  -7.230412483215332,\n",
       "  -7.480142593383789,\n",
       "  -7.032469749450684,\n",
       "  -7.411925315856934,\n",
       "  -7.4671454429626465,\n",
       "  -7.796328544616699,\n",
       "  -7.102053165435791,\n",
       "  -7.488613128662109,\n",
       "  -7.081622123718262,\n",
       "  -7.403144836425781,\n",
       "  -7.07993221282959,\n",
       "  -7.30258321762085,\n",
       "  -7.194300651550293,\n",
       "  -7.03626823425293,\n",
       "  -7.041921615600586,\n",
       "  -6.975368499755859,\n",
       "  -7.289743423461914,\n",
       "  -7.578144073486328,\n",
       "  -7.2994184494018555,\n",
       "  -7.152080059051514,\n",
       "  -7.561187267303467,\n",
       "  -7.3396711349487305,\n",
       "  -6.919654369354248,\n",
       "  -7.196962833404541,\n",
       "  -7.267926216125488,\n",
       "  -7.68022346496582,\n",
       "  -6.981371879577637,\n",
       "  -7.325427055358887,\n",
       "  -7.269104480743408,\n",
       "  -7.193674087524414,\n",
       "  -7.3791279792785645,\n",
       "  -7.001983165740967,\n",
       "  -6.749326705932617,\n",
       "  -6.835741996765137,\n",
       "  -6.9042205810546875,\n",
       "  -7.133326530456543,\n",
       "  -7.623200416564941,\n",
       "  -7.223666191101074,\n",
       "  -7.208978176116943,\n",
       "  -7.81057071685791,\n",
       "  -7.135682582855225,\n",
       "  -7.338241100311279,\n",
       "  -7.558967590332031,\n",
       "  -7.240979194641113,\n",
       "  -7.218428134918213,\n",
       "  -7.093049049377441,\n",
       "  -7.312836647033691,\n",
       "  -7.153634548187256,\n",
       "  -7.419790267944336,\n",
       "  -7.384675025939941,\n",
       "  -7.460193157196045,\n",
       "  -7.135217189788818,\n",
       "  -7.024266719818115,\n",
       "  -7.39983606338501,\n",
       "  -7.338173866271973,\n",
       "  -7.67953634262085,\n",
       "  -6.3855719566345215,\n",
       "  -7.123971462249756,\n",
       "  -7.353171348571777,\n",
       "  -7.157285213470459,\n",
       "  -7.446743965148926,\n",
       "  -7.572507381439209,\n",
       "  -7.526236534118652,\n",
       "  -7.389542579650879,\n",
       "  -7.80484676361084,\n",
       "  -7.545668125152588,\n",
       "  -7.283243179321289,\n",
       "  -7.2537055015563965,\n",
       "  -7.34984016418457,\n",
       "  -7.671350955963135,\n",
       "  -7.540934085845947,\n",
       "  -7.601498603820801,\n",
       "  -7.151137351989746,\n",
       "  -7.259981632232666,\n",
       "  -7.23912239074707,\n",
       "  -7.272661209106445,\n",
       "  -7.339310169219971,\n",
       "  -7.4170427322387695,\n",
       "  -7.275513172149658,\n",
       "  -7.007856845855713,\n",
       "  -7.385669231414795,\n",
       "  -7.182992458343506,\n",
       "  -7.410045146942139,\n",
       "  -7.242282867431641,\n",
       "  -7.189862251281738,\n",
       "  -7.1471405029296875,\n",
       "  -7.7178955078125,\n",
       "  -7.106612205505371,\n",
       "  -7.357628345489502,\n",
       "  -6.884399890899658,\n",
       "  -7.579459190368652,\n",
       "  -7.2440385818481445,\n",
       "  -7.139525413513184,\n",
       "  -6.980842590332031,\n",
       "  -7.067513465881348,\n",
       "  -7.579305171966553,\n",
       "  -7.834651947021484,\n",
       "  -7.1634087562561035,\n",
       "  -7.296807289123535,\n",
       "  -6.992648124694824,\n",
       "  -7.796300411224365,\n",
       "  -7.119023323059082,\n",
       "  -7.12498664855957,\n",
       "  -7.4599480628967285,\n",
       "  -7.572044372558594,\n",
       "  -7.580385208129883,\n",
       "  -7.910735130310059,\n",
       "  -7.401893138885498,\n",
       "  -7.017055511474609,\n",
       "  -7.285894393920898,\n",
       "  -7.193017959594727,\n",
       "  -6.951793670654297,\n",
       "  -7.736076354980469,\n",
       "  -6.895786285400391,\n",
       "  -7.322038173675537,\n",
       "  -7.3633952140808105,\n",
       "  -7.249682426452637,\n",
       "  -7.177511692047119,\n",
       "  -6.9532976150512695,\n",
       "  -7.200125694274902,\n",
       "  -7.543424606323242,\n",
       "  -7.234555721282959,\n",
       "  -7.2478156089782715,\n",
       "  -7.1801862716674805,\n",
       "  -7.349573135375977,\n",
       "  -7.352355480194092,\n",
       "  -7.2119975090026855,\n",
       "  -7.3987016677856445,\n",
       "  -7.520451545715332,\n",
       "  -7.3632707595825195,\n",
       "  -7.562435150146484,\n",
       "  -7.410613536834717,\n",
       "  -7.5278472900390625,\n",
       "  -7.4025068283081055,\n",
       "  -7.511164665222168,\n",
       "  -7.239932060241699,\n",
       "  -7.238926887512207,\n",
       "  -7.59418249130249,\n",
       "  -7.335660457611084,\n",
       "  -7.326104164123535,\n",
       "  -7.023022651672363,\n",
       "  -6.665472984313965,\n",
       "  -7.0610198974609375,\n",
       "  -7.381904125213623,\n",
       "  -7.254326820373535,\n",
       "  -6.916460990905762,\n",
       "  -7.219583988189697,\n",
       "  -7.2971906661987305,\n",
       "  -6.9220991134643555,\n",
       "  -7.054749488830566,\n",
       "  -7.275244235992432,\n",
       "  -7.3783278465271,\n",
       "  -7.275266170501709,\n",
       "  -7.588473320007324,\n",
       "  -7.54753303527832,\n",
       "  -7.193815231323242,\n",
       "  -7.436957359313965,\n",
       "  -7.441195487976074,\n",
       "  -7.37455940246582,\n",
       "  -7.249761581420898,\n",
       "  -7.087191104888916,\n",
       "  -6.734531879425049,\n",
       "  -6.966699600219727,\n",
       "  -7.383293151855469,\n",
       "  -7.571385383605957,\n",
       "  -7.144279479980469,\n",
       "  -7.097696304321289,\n",
       "  -7.378757953643799,\n",
       "  -7.009606838226318,\n",
       "  -7.579474449157715,\n",
       "  -7.796772003173828,\n",
       "  -7.495147228240967,\n",
       "  -7.3123650550842285,\n",
       "  -7.354918003082275,\n",
       "  -7.116016387939453,\n",
       "  -7.123875141143799,\n",
       "  -7.066173553466797,\n",
       "  -7.182529449462891,\n",
       "  -7.068865776062012,\n",
       "  -7.426568984985352,\n",
       "  -7.217582702636719,\n",
       "  -7.1612653732299805,\n",
       "  -7.558455467224121,\n",
       "  -7.421079158782959,\n",
       "  -7.624800682067871,\n",
       "  -6.844422340393066,\n",
       "  -7.450662612915039,\n",
       "  -7.69474458694458,\n",
       "  -6.910746097564697,\n",
       "  -6.9137349128723145,\n",
       "  -7.2014265060424805,\n",
       "  -7.00230598449707,\n",
       "  -7.150501728057861,\n",
       "  -7.414660453796387,\n",
       "  -7.215671062469482,\n",
       "  -7.349539756774902,\n",
       "  -7.05826997756958,\n",
       "  -7.038989067077637,\n",
       "  -7.160373687744141,\n",
       "  -7.245350360870361,\n",
       "  -7.4148969650268555,\n",
       "  -7.166001319885254,\n",
       "  -7.7779388427734375,\n",
       "  -7.236688613891602,\n",
       "  -7.430649280548096,\n",
       "  -7.515650749206543,\n",
       "  -7.110456943511963,\n",
       "  -6.930889129638672,\n",
       "  -7.280766010284424,\n",
       "  -7.245162010192871,\n",
       "  -7.111240386962891,\n",
       "  -7.065334320068359,\n",
       "  -7.48039436340332,\n",
       "  -7.69625186920166,\n",
       "  -6.969426155090332,\n",
       "  -7.113555908203125,\n",
       "  -7.194823741912842,\n",
       "  -6.971942901611328,\n",
       "  -7.117112159729004,\n",
       "  -6.920730113983154,\n",
       "  -7.577777862548828,\n",
       "  -7.132415771484375,\n",
       "  -7.568923473358154,\n",
       "  -7.429105758666992,\n",
       "  -7.237141132354736,\n",
       "  -7.546688556671143,\n",
       "  -7.35819673538208,\n",
       "  -7.317974090576172,\n",
       "  -7.138570308685303,\n",
       "  -7.241734027862549,\n",
       "  -7.391797065734863,\n",
       "  -7.142148017883301,\n",
       "  -7.599236488342285,\n",
       "  -7.715930938720703,\n",
       "  -7.141814231872559,\n",
       "  -7.276235580444336,\n",
       "  -7.288337230682373,\n",
       "  -7.4187188148498535,\n",
       "  -7.260677814483643,\n",
       "  -8.028594970703125,\n",
       "  -7.523538589477539,\n",
       "  -7.404396057128906,\n",
       "  -7.183443546295166,\n",
       "  -7.391782283782959,\n",
       "  -6.9404802322387695,\n",
       "  -7.4605607986450195,\n",
       "  -6.90075159072876,\n",
       "  -7.2565131187438965,\n",
       "  -7.471352577209473,\n",
       "  -7.636147499084473,\n",
       "  -7.527283668518066,\n",
       "  -6.808420658111572,\n",
       "  -7.252614974975586,\n",
       "  -7.554294586181641,\n",
       "  -7.0547027587890625,\n",
       "  -7.280166149139404,\n",
       "  -7.187770843505859,\n",
       "  -7.129264831542969,\n",
       "  -7.186864376068115,\n",
       "  -7.045409202575684,\n",
       "  -7.575051784515381,\n",
       "  -7.024353504180908,\n",
       "  -6.747942924499512,\n",
       "  -7.2190842628479,\n",
       "  -7.192834854125977,\n",
       "  -7.396596431732178,\n",
       "  -7.4766387939453125,\n",
       "  -7.229710578918457,\n",
       "  -7.0705342292785645,\n",
       "  -7.445225238800049,\n",
       "  -7.47822380065918,\n",
       "  -7.354962348937988,\n",
       "  -7.132996559143066,\n",
       "  -7.556343078613281,\n",
       "  -7.512652397155762,\n",
       "  -7.036288261413574,\n",
       "  -7.464210510253906,\n",
       "  -7.20064640045166,\n",
       "  -7.389324188232422,\n",
       "  -7.136985778808594,\n",
       "  -7.291080474853516,\n",
       "  -7.458184242248535,\n",
       "  -7.182502269744873,\n",
       "  -7.262008190155029,\n",
       "  -7.415464878082275,\n",
       "  -7.07611608505249,\n",
       "  -7.580387115478516,\n",
       "  -7.424394607543945,\n",
       "  -7.245307922363281,\n",
       "  -7.114466667175293,\n",
       "  -7.245833873748779,\n",
       "  -7.215500354766846,\n",
       "  -7.300826072692871,\n",
       "  -7.521548271179199,\n",
       "  -7.232094764709473,\n",
       "  -7.4259772300720215,\n",
       "  -7.0273966789245605,\n",
       "  -7.256992816925049,\n",
       "  -6.991838455200195,\n",
       "  -7.449475288391113,\n",
       "  -7.4720306396484375,\n",
       "  -6.831389904022217,\n",
       "  -6.905251502990723,\n",
       "  -7.334801197052002,\n",
       "  -7.2749457359313965,\n",
       "  -6.9351115226745605,\n",
       "  -7.190101623535156,\n",
       "  -7.512868881225586,\n",
       "  -7.386318683624268,\n",
       "  -6.776792526245117,\n",
       "  -6.938546180725098,\n",
       "  -7.461359977722168,\n",
       "  -7.469109535217285,\n",
       "  -6.891740798950195,\n",
       "  -7.147923469543457,\n",
       "  -7.3826141357421875,\n",
       "  -7.762337684631348,\n",
       "  -6.92588996887207,\n",
       "  -7.3027472496032715,\n",
       "  -7.3388237953186035,\n",
       "  -7.039022922515869,\n",
       "  -7.352632522583008,\n",
       "  -7.521681785583496,\n",
       "  -7.016263961791992,\n",
       "  -6.922280311584473,\n",
       "  -7.331243515014648,\n",
       "  -7.31251335144043,\n",
       "  -6.829278945922852,\n",
       "  -7.520852088928223,\n",
       "  -7.253537654876709,\n",
       "  -7.126461505889893,\n",
       "  -7.875966548919678,\n",
       "  -7.434423446655273,\n",
       "  -7.118742942810059,\n",
       "  -7.663218021392822,\n",
       "  -7.3719258308410645,\n",
       "  -7.278858184814453,\n",
       "  -6.764481067657471,\n",
       "  -7.393847465515137,\n",
       "  -6.924572467803955,\n",
       "  -6.73823356628418,\n",
       "  -7.420225620269775,\n",
       "  -7.613102912902832,\n",
       "  -7.180402755737305,\n",
       "  -7.310311794281006,\n",
       "  -7.690427303314209,\n",
       "  -6.9494242668151855,\n",
       "  -6.5617265701293945,\n",
       "  -6.852583408355713,\n",
       "  -7.539813995361328,\n",
       "  -7.144554615020752,\n",
       "  -7.053829669952393,\n",
       "  -7.021794319152832,\n",
       "  -7.421758651733398,\n",
       "  -7.389595031738281,\n",
       "  -7.270789623260498,\n",
       "  -7.340378284454346,\n",
       "  -7.279489517211914,\n",
       "  -7.547577857971191,\n",
       "  -7.449851989746094,\n",
       "  -7.236766815185547,\n",
       "  -6.888304710388184,\n",
       "  -7.446091175079346,\n",
       "  -7.129983425140381,\n",
       "  -7.161524772644043,\n",
       "  -7.474789619445801,\n",
       "  -7.028975009918213,\n",
       "  -6.957800388336182,\n",
       "  -7.182094097137451,\n",
       "  -6.767513751983643,\n",
       "  -7.352543830871582,\n",
       "  -7.332246780395508,\n",
       "  -7.123093605041504,\n",
       "  -7.621530055999756,\n",
       "  -6.593232154846191,\n",
       "  -6.795840740203857,\n",
       "  -6.812492370605469,\n",
       "  -7.265992641448975,\n",
       "  -7.261761665344238,\n",
       "  -7.2351179122924805,\n",
       "  -6.91131591796875,\n",
       "  -7.362796783447266,\n",
       "  -6.9071574211120605,\n",
       "  -7.157345771789551,\n",
       "  -7.512063026428223,\n",
       "  -7.160690784454346,\n",
       "  -7.171656608581543,\n",
       "  -7.164365291595459,\n",
       "  -7.179537296295166,\n",
       "  -7.1426873207092285,\n",
       "  -7.611781597137451,\n",
       "  -6.862739562988281,\n",
       "  -7.0904388427734375,\n",
       "  -7.463034629821777,\n",
       "  -7.31452751159668,\n",
       "  -6.7123517990112305,\n",
       "  -7.3689985275268555,\n",
       "  -7.256990432739258,\n",
       "  -7.357952117919922,\n",
       "  -7.237146854400635,\n",
       "  -7.101307392120361,\n",
       "  -7.099776744842529,\n",
       "  -7.2040629386901855,\n",
       "  -7.209155082702637,\n",
       "  -7.105715751647949,\n",
       "  -7.530524253845215,\n",
       "  -7.2715630531311035,\n",
       "  -7.315462589263916,\n",
       "  -7.33588171005249,\n",
       "  -7.8446044921875,\n",
       "  -7.207254409790039,\n",
       "  -6.896467685699463,\n",
       "  -7.50558614730835,\n",
       "  -7.168217658996582,\n",
       "  -7.068119049072266,\n",
       "  -7.373969554901123,\n",
       "  -7.108559608459473,\n",
       "  -7.587578296661377,\n",
       "  -7.532353401184082,\n",
       "  -7.506638050079346,\n",
       "  -7.460268020629883,\n",
       "  -7.455312252044678,\n",
       "  -7.393716812133789,\n",
       "  -7.398367881774902,\n",
       "  -7.465319633483887,\n",
       "  -7.315398693084717,\n",
       "  -7.2599406242370605,\n",
       "  -6.956275939941406,\n",
       "  -6.933394908905029,\n",
       "  -7.295968532562256,\n",
       "  -7.283629417419434,\n",
       "  -7.323081016540527,\n",
       "  -7.210507392883301,\n",
       "  -7.257829189300537,\n",
       "  -7.0333476066589355,\n",
       "  -7.318506717681885,\n",
       "  -7.335518836975098,\n",
       "  -7.238780975341797,\n",
       "  -7.782773017883301,\n",
       "  -7.350992679595947,\n",
       "  -7.429154396057129,\n",
       "  -7.383627891540527,\n",
       "  -7.022735118865967,\n",
       "  -7.354201793670654,\n",
       "  -7.762507438659668,\n",
       "  -6.968134880065918,\n",
       "  -7.703325271606445,\n",
       "  -7.371733665466309,\n",
       "  -7.315505027770996,\n",
       "  -6.8105597496032715,\n",
       "  -7.155302047729492,\n",
       "  -7.558388710021973,\n",
       "  -7.044817924499512,\n",
       "  -6.857080459594727,\n",
       "  -7.323685169219971,\n",
       "  -7.103482246398926,\n",
       "  -6.959583282470703,\n",
       "  -6.915165901184082,\n",
       "  -7.189262390136719,\n",
       "  -7.378617286682129,\n",
       "  -7.296328544616699,\n",
       "  -6.990835189819336,\n",
       "  -6.662012100219727,\n",
       "  -7.022043228149414,\n",
       "  -7.331607818603516,\n",
       "  -7.341364860534668,\n",
       "  -6.8902482986450195,\n",
       "  -7.373333930969238,\n",
       "  -7.11807107925415,\n",
       "  -7.567612648010254,\n",
       "  -7.566336154937744,\n",
       "  -7.502502918243408,\n",
       "  -7.2517499923706055,\n",
       "  -7.025180816650391,\n",
       "  -7.272538185119629,\n",
       "  -7.603973388671875,\n",
       "  -7.263160228729248,\n",
       "  -7.461052894592285,\n",
       "  -6.861146926879883,\n",
       "  -7.024476528167725,\n",
       "  -7.736571788787842,\n",
       "  -7.242768287658691,\n",
       "  -7.538966655731201,\n",
       "  -7.68983268737793,\n",
       "  -7.247715950012207,\n",
       "  -7.624255657196045,\n",
       "  -7.2729339599609375,\n",
       "  -7.271572113037109,\n",
       "  -7.549345970153809,\n",
       "  -6.945012092590332,\n",
       "  -6.894528388977051,\n",
       "  -7.173004150390625,\n",
       "  -7.167106628417969,\n",
       "  -7.605728626251221,\n",
       "  -7.311908721923828,\n",
       "  -7.557284832000732,\n",
       "  -7.37575626373291,\n",
       "  -7.043959617614746,\n",
       "  -7.298795223236084,\n",
       "  -7.3506927490234375,\n",
       "  -7.306920051574707,\n",
       "  -7.266623497009277,\n",
       "  -7.408327579498291,\n",
       "  -7.49491024017334,\n",
       "  -7.341501235961914,\n",
       "  -7.098458766937256,\n",
       "  -7.419225692749023,\n",
       "  -7.131242752075195,\n",
       "  -7.237518310546875,\n",
       "  -7.15363073348999,\n",
       "  -7.028456687927246,\n",
       "  -7.348419666290283,\n",
       "  -6.8525919914245605,\n",
       "  -7.537107944488525,\n",
       "  -7.521017551422119,\n",
       "  -7.015052318572998,\n",
       "  -6.8282365798950195,\n",
       "  -7.22776460647583,\n",
       "  -7.101095199584961,\n",
       "  -7.2573957443237305,\n",
       "  -7.0330705642700195,\n",
       "  -6.99738883972168,\n",
       "  -7.290280342102051,\n",
       "  -7.169556617736816,\n",
       "  -7.406413555145264,\n",
       "  -7.240409851074219,\n",
       "  -7.415460586547852,\n",
       "  -7.361958026885986,\n",
       "  -7.489359378814697,\n",
       "  -7.246927261352539,\n",
       "  -7.277602672576904,\n",
       "  -7.114871025085449,\n",
       "  -7.230363368988037,\n",
       "  -6.926900386810303,\n",
       "  -7.647730827331543,\n",
       "  -7.300573348999023,\n",
       "  -7.494931697845459,\n",
       "  -7.0248332023620605,\n",
       "  -7.904386520385742,\n",
       "  -7.192670822143555,\n",
       "  -7.936649799346924,\n",
       "  -7.567332744598389,\n",
       "  -7.1888108253479,\n",
       "  -7.012898921966553,\n",
       "  -7.228670120239258,\n",
       "  -7.463482856750488,\n",
       "  -7.156089782714844,\n",
       "  -7.651949882507324,\n",
       "  -7.068199157714844,\n",
       "  -7.37583065032959,\n",
       "  -7.3186516761779785,\n",
       "  -7.285305500030518,\n",
       "  -7.346488952636719,\n",
       "  -6.910711288452148,\n",
       "  -7.396983623504639,\n",
       "  -7.244119644165039,\n",
       "  -7.373660087585449,\n",
       "  -7.508713245391846,\n",
       "  -6.9587931632995605,\n",
       "  -7.323297023773193,\n",
       "  -7.175942897796631,\n",
       "  -7.399770736694336,\n",
       "  -7.043058395385742,\n",
       "  -7.428898334503174,\n",
       "  -7.045318603515625,\n",
       "  -7.3524322509765625,\n",
       "  -7.368280410766602,\n",
       "  -7.317091464996338,\n",
       "  -7.412913799285889,\n",
       "  -7.452236175537109,\n",
       "  -7.873819351196289,\n",
       "  -7.39579439163208,\n",
       "  -7.254408836364746,\n",
       "  -7.32444953918457,\n",
       "  -7.301690101623535,\n",
       "  -7.605166435241699,\n",
       "  -6.944730758666992,\n",
       "  -7.308675765991211,\n",
       "  ...]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.LogSoftmax(dim=1)\n",
    "softmax(lin2(h_x)).detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRQWuqqEHAOy",
    "outputId": "b415f713-540a-4ea5-c47e-37119170e1a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-6.3856], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([478]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the index with highest softmax probabilities\n",
    "# See https://pytorch.org/docs/stable/torch.html#torch.max\n",
    "torch.max(softmax(lin2(h_x)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ff9AI3AzHAO1"
   },
   "source": [
    "<a id=\"section-3-1-4-train-cbow\"></a>\n",
    "\n",
    "# Now, we train the CBOW model for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KajporK5HAO1",
    "outputId": "64a80e84-7ccf-430e-b754-453ead42ccb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 24)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we split the data into training and testing.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
    "len(tokenized_text_train), len(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_e-DyvAPHAO3"
   },
   "outputs": [],
   "source": [
    "### Hint: Click here to go back up to see the CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNV7M6HiHAO3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Put the input context indices into the embeddings\n",
    "        # then squeeze it into a single dimension vector with tensor.view((1,-1))\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        # Put the embedding input through linear layer,\n",
    "        # then an activation function to create the hidden layer.\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        # Put the hidden layer through a second linear layer,\n",
    "        out = self.linear2(hid)\n",
    "        # then a last layer activation function to generate\n",
    "        # pobabilities, hint https://pytorch.org/docs/stable/nn.html#torch.nn.functional.log_softmax\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyJ3a03DHAO6",
    "outputId": "a74a8ea3-91e3-42a7-ef0d-6f49b63b60e2"
   },
   "outputs": [],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "window_size = 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, \n",
    "                           window_size=window_size, \n",
    "                           variant='cbow')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "# Hint: the CBOW model object you've created.\n",
    "model = CBOW(vocab_size, embd_size, window_size, hidden_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:09<15:37,  9.47s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d0d0b5219923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Save model after every epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Zero gradient.\n",
    "            optimizer.zero_grad()\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x, y = w2v_io['x'], w2v_io['y']\n",
    "            x = tensor(x).to(device)\n",
    "            y = autograd.Variable(tensor(y, dtype=torch.long)).to(device)\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8HrtnnIHAO6",
    "outputId": "cc6cc1b7-bf8a-47c0-ee9e-27df461e114d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qc82I7eiHAO8"
   },
   "source": [
    "<a id=\"section-3-1-4-evaluate-cbow\"></a>\n",
    "\n",
    "# Apply and Evaluate the CBOW Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRk8qOwUHAO8",
    "outputId": "6a6cd788-b5e1-4dc7-a0e2-bbee3e053bf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[91mof\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mthe\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91m______\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91m______\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mis\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[91mand\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91m______\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[91m______\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91m______\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91m______\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91m______\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[92mis\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mthe\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mthe\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91m______\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91m______\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[91m______\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91m______\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91m______\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91m______\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mand\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mthe\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91m______\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mand\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mthe\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91mthe\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mthe\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mthe\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91mthe\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mthe\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91m(\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91m______\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91m______\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m______\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91m______\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mthe\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91m______\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91m______\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91m______\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91m______\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91m______\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mof\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91m______\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91m______\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mis\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91m______\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mof\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91m______\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mis\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mis\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[91m______\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[91m______\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91min\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91m______\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mof\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91m______\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mthe\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91m______\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91m______\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91m______\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[91m______\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mof\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[92mthe\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91m______\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mof\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mis\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91m______\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91m______\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mthe\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m______\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91m______\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91m______\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91m______\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mis\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91m______\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91m______\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mof\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91m______\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[91m______\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91m______\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mthe\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91m______\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91m______\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91m______\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mthe\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[91m______\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mthe\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mthe\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91m)\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mthe\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mthe\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mthe\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91m______\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[91mand\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m______\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[91m______\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mthe\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91m______\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91m)\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91m______\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[91mand\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[91mthe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mthe\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mof\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[91m______\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[91mis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[91mis\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[91m______\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[91m______\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[91mthe\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mthe\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[91m______\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[91m______\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[91m______\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[91m______\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[91mof\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[91m______\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[91mis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mthe\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91m______\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91m______\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[91m______\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[91mthe\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[91m______\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mthe\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91m______\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91m______\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mthe\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91m______\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91m______\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[91m______\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91m______\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mthe\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91m______\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91m______\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91m______\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91m______\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91m______\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91m(\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mthe\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91m______\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91m______\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mof\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[91m______\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[91m______\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mof\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91m______\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91m______\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mof\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mthe\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91m______\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mof\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mof\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mthe\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mthe\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mthe\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91m______\u001b[0m , as\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[91mis\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91mof\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91mof\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91m______\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthe\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[91m______\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[91mand\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[91mthe\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91m______\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mof\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mand\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91m______\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[92mthe\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91m(\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mis\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mthe\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[91m______\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[91m______\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91mthe\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91m)\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91m______\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mthe\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthe\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mof\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91m______\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91m______\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mthe\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mand\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91m______\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91m______\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mthe\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91m______\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x']).to(device)\n",
    "        y = tensor(w2v_io['y']).to(device)\n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Remember how to get the best prediction output? \n",
    "            # Hint: https://pytorch.org/docs/stable/torch.html#torch.max\n",
    "            _, prediction =  torch.max(model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_dataset.vocab.doc2idx(['blahlablahblah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PelknL2MHAO9",
    "outputId": "07bf09ce-6bd3-445d-93b9-8d323e4536ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-2.2899], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([0]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model(x), 1)\n",
    "#model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LFxlL7a-HAO-",
    "outputId": "d9b07963-2da0-488b-fb44-8138e2cda4e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.12340425531914893\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfJ52Vf3HAO_"
   },
   "source": [
    "<a id=\"section-3-1-4-load-model\"></a>\n",
    "\n",
    "# Go back to the 5th Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLyxuoUCHAO_",
    "outputId": "8c7afdee-a14e-4252-90c9-27b661c0d1a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CBOW(\n",
       "    (embeddings): Embedding(1303, 100)\n",
       "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=1303, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "model_5 = torch.nn.DataParallel(model_5)\n",
    "model_5.load_state_dict(torch.load('cbow_checkpoint_5.pt'))\n",
    "model_5.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvy7BIcaHAPB",
    "outputId": "08f74835-bc89-4a1b-bab7-72a7cd2dadde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mto\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mof\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mfor\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[91mwe\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91mrandom\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[91m______\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[92mor\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mrandom\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91m______\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91mof\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mnot\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mrandom\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mof\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91m______\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mtwo\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mand\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mnot\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mrandom\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mcorrell\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mand\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91min\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mever\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91mrandom\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mstatistics\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mand\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91mto\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mwe\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mto\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mbe\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mbe\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m______\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mthe\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91m______\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91m______\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mtwo\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mof\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mof\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mof\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mbe\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mis\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mto\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mand\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mof\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[92mthe\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mof\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mto\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[91mand\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91min\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mmle\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mto\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91m______\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mfor\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91m1\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91m)\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mand\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mreject\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mthat\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91muse\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mand\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mof\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mto\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mnull\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mor\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91mbetween\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mnumber\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mand\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[92mlanguage\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mtwo\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mcorpora\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mto\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mto\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mrandom\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthis\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91m)\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthat\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mand\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mwords\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mtwo\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91muse\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mand\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mand\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mbe\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91mof\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[91mand\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m2\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mand\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mthe\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mdata\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91m______\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[91mas\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[91m______\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mshall\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mof\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[91mis\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mwe\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[92mhave\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91mthe\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mwhich\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mlanguage\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mthe\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mthere\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mfor\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mthe\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mand\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91mis\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mword\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91mand\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mto\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mvery\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mand\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mand\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mdoes\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mvery\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mword\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mrandom\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91m______\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mwith\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[91mnull\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[91mlanguage\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mfor\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mvery\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mlinguistics\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91m______\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mthat\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mof\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mreject\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mof\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91mrandom\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfor\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mthe\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mand\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mis\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91m)\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mas\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mthe\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91mis\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mis\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[92mlanguage\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mlanguage\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91min\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mrandom\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mare\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mfor\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91ma\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[92merror\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mof\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mis\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[91mto\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91m______\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mnumber\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91m)\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mdata\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mand\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mto\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mat\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91m______\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mthe\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mwe\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91ma\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mthe\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mto\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mfrom\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mof\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x']).to(device)\n",
    "        y = tensor(w2v_io['y']).to(device)\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model_5(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YUJqPZ7HAPB",
    "outputId": "5a656132-a709-446a-fc9e-d80de50149b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.23404255319148937\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahfez391HAPC",
    "outputId": "ca734d53-ad82-4bd9-ab89-7acad3b3865a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape[1] == len(w2v_dataset.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byBffEPbHAPD"
   },
   "source": [
    "# [optional] How to Handle Unknown Words? \n",
    "\n",
    "This is not the best way to handle unknown words, but we can simply assign an index for unknown words.\n",
    "\n",
    "**Hint:** Ensure that you have `gensim` version >= 3.7.0 first. Otherwise this part of the code won't work. \n",
    "\n",
    "Try in your Python environment installation:\n",
    "\n",
    "```\n",
    "python -m pip install -U pip\n",
    "python -m pip install -U gensim>=3.7.0\n",
    "```\n",
    "\n",
    "Or within the jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXrkv7c7HAPD",
    "outputId": "990513e6-3892-4141-eef3-aeed109069ec"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install -U pip\n",
    "!python3 -m pip install -U gensim>=3.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pvz2r2H7HAPF"
   },
   "source": [
    "To check version of `gensim` after installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQ2fXMy3HAPG",
    "outputId": "7faf4593-1f8b-4048-bf3e-65c8f3748653"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.1'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lr7AsumYHAPH",
    "outputId": "0536f940-f7a0-4277-eb45-d78f6146d15d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eLLNGLkmHAPH",
    "outputId": "02b4fc5f-7a07-4a85-a1c5-11b3dd425f06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: 'a',\n",
       " 7: 'bar',\n",
       " 2: 'foo',\n",
       " 3: 'is',\n",
       " 4: 'sentence',\n",
       " 5: 'this',\n",
       " 0: '<pad>',\n",
       " 1: '<unk>'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens\n",
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "\n",
    "try:\n",
    "    special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "    vocab.patch_with_special_tokens(special_tokens)\n",
    "except: # If gensim is not 3.7.0\n",
    "    pass\n",
    "    \n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mSN-D1_HAPI"
   },
   "source": [
    "# [optional] Lets Rewrite the `Word2VecText` Object\n",
    "\n",
    "Now with the (i) unknown word patch in the vocabulary as well as (ii) `skipgram_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiHLsrp6HAPI"
   },
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Add the unknown word patch here.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        try:\n",
    "            special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "            self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZYpe5oyHAPK"
   },
   "source": [
    "<a id=\"section-3-1-5\"></a>\n",
    "\n",
    "# Lets try the skipgram task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDLnpSjWHAPK"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSiH5MPkHAPK"
   },
   "source": [
    "<a id=\"section-3-1-5-foward\"></a>\n",
    "\n",
    "# Take a closer look at what's in the `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xyw44fdTHAPL"
   },
   "outputs": [],
   "source": [
    "xx1 = torch.rand(1,20)\n",
    "xx2 = torch.rand(1,20)\n",
    "\n",
    "xx1_numpy = xx1.detach().numpy()\n",
    "xx2_numpy = xx2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TaDPAuwHAPN",
    "outputId": "31359962-d996-45dc-c33b-53ad2d2fc7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "(20, 1)\n",
      "[[5.287307]]\n"
     ]
    }
   ],
   "source": [
    "print(xx1_numpy.shape)\n",
    "print(xx2_numpy.T.shape)\n",
    "print(np.dot(xx1_numpy, xx2_numpy.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlm-KSkfHAPQ",
    "outputId": "e54c0fc5-2b98-42be-d8f6-a750c6616ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([20, 1])\n",
      "tensor([[5.2873]])\n"
     ]
    }
   ],
   "source": [
    "print(xx1.shape)\n",
    "print(torch.t(xx2).shape) \n",
    "\n",
    "print(torch.mm(xx1, torch.t(xx2))) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7I5p6iiHAPQ"
   },
   "source": [
    "<a id=\"section-3-1-5-train\"></a>\n",
    "\n",
    "# Train a Skipgram model (for real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARKBllDSHAPR",
    "outputId": "837743f3-e00c-4e10-cdc8-ea73ae3fa794"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 1/100 [00:53<1:28:04, 53.38s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-16ece259c2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# This unsqueeze thing is really a feature/bug... -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mepcoh_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.03\n",
    "window_size = 3\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=3, variant='skipgram')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# Use the Skipgram object\n",
    "model = SkipGram(vocab_size, embd_size,).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epcoh_loss = 0\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x1, x2 = w2v_io['x']\n",
    "            x1, x2 = tensor(x1).to(device), tensor(x2).to(device)\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.float)).to(device)\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x1, x2)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.view(1, -1)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epcoh_loss += float(loss)\n",
    "    torch.save(model.state_dict(), 'skipgram_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(epcoh_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# `logprobs` is in the shape of [1, 1]\n",
    "print(logprobs.shape)\n",
    "\n",
    "# `y` is in the shape of [] , i.e. just a float not in an array.\n",
    "print(y.shape)\n",
    "\n",
    "# To make `y` to be the shape of the `logprobs`, we use .unsqueeze() twice\n",
    "# Note: This might change with varying torch version... -_-\n",
    "print(y.unsqueeze(0).unsqueeze(0).shape)\n",
    "\n",
    "# Alternatively, you can force the tensor into specific shape with .view()\n",
    "# https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "# Strongly, encourage you to do some \"tensor-fu\", on \n",
    "#  - https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/\n",
    "print(y.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISCj45lKHAPT"
   },
   "source": [
    "<a id=\"section-3-1-5-evaluate\"></a>\n",
    "\n",
    "# Evaluate the model on the skipgram task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "590qbKawHAPU"
   },
   "outputs": [],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        _, prediction =  torch.max(model(x1, x2), 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyFGwLo0HAPW",
    "outputId": "34adc1fd-1f7b-4884-ba3c-b749599885f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S1fV59wHAPY"
   },
   "source": [
    "## Download the Collobert and Weston SENNA Embeddings\n",
    "\n",
    "\n",
    "If you're on a Mac or Linux, you can use the `!` bang commands in the next cell to get the data.\n",
    "\n",
    "```\n",
    "!pip install kaggle\n",
    "!mkdir -p .kaggle\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > .kaggle/kaggle.json\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./\n",
    "```\n",
    "\n",
    "If you're on windows go to https://www.kaggle.com/alvations/vegetables-senna-embeddings and download the data files. \n",
    "\n",
    "What's most important are the \n",
    " - `.txt` file that contains the vocabulary list\n",
    " - `.npy` file that contains the binarized numpy array\n",
    " \n",
    "The rows of the numpy array corresponds to the vocabulary in the order from the `.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGoQhZaUHAPZ"
   },
   "source": [
    "<a id=\"section-3-1-6-vocab\"></a>\n",
    "\n",
    "\n",
    "## 3.1.6. Loading Pre-trained Embeddings\n",
    "\n",
    "Lets overwrite the `Word2VecText` object with the pretrained embeddings. \n",
    "\n",
    "Most important thing is the overwrite the `Dictionary` from `gensim` with the vocabulary of the pre-trained embeddings, as such:\n",
    "\n",
    "```python\n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgibLmBxHAPZ"
   },
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=-1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXnuGFTlHAPZ"
   },
   "source": [
    "<a id=\"section-3-1-6-pretrained\"></a>\n",
    "\n",
    "## Override the embeddings layer with the pre-trained weights.\n",
    "\n",
    "In PyTorch, the weights of the `nn.Embedding` object can be easily overwritten with `from_pretrained` function, see https://pytorch.org/docs/stable/nn.html#embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpnd3WCjHAPc"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, pretrained_npy):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        # Put the index of the focus word into the embedding layer.\n",
    "        embed_focus = self.???(focus).view((1, -1))\n",
    "        # Put the index of the context word into the embedding layer.\n",
    "        embed_context = self.???(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        # Do a matrix multiplication between the focus and context embedding\n",
    "        score = ???(embed_focus, torch.t(embed_context))\n",
    "        # Then put it through a log sigmoid activation function\n",
    "        # so that the output is between (log(0), log(1))\n",
    "        log_probs = F.???(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-i2qQprHAPe",
    "outputId": "be80b816-fa74-4003-efa8-b757f4170491"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.03682 ,  1.77856 , -0.693547, ..., -0.10278 , -0.36428 ,\n",
       "        -0.64853 ],\n",
       "       [-2.19067 ,  1.16642 , -1.91385 , ...,  0.870654, -0.33808 ,\n",
       "        -0.41957 ],\n",
       "       [ 1.16672 ,  0.811884, -0.115492, ..., -0.104843,  2.26862 ,\n",
       "         1.21729 ],\n",
       "       ...,\n",
       "       [-0.483488,  2.00359 ,  0.186266, ..., -0.114528,  1.50755 ,\n",
       "        -1.25606 ],\n",
       "       [ 0.201604,  1.15796 ,  0.888882, ..., -1.28183 ,  0.465847,\n",
       "        -1.57974 ],\n",
       "       [-0.238824,  0.443876,  0.290836, ..., -0.802705, -0.318169,\n",
       "        -1.4733  ]])"
      ]
     },
     "execution_count": 183,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('senna.wiki-reuters.lm2.50d.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rHad7RtHAPf"
   },
   "outputs": [],
   "source": [
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
    "pretrained_npy = torch.tensor(np.load('senna.wiki-reuters.lm2.50d.npy'))\n",
    "pretrained_model = SkipGram(pretrained_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSYkwNJLHAPf"
   },
   "source": [
    "<a id=\"section-3-1-6-eval-skipgram\"></a>\n",
    "## Test Pretrained Embeddings on the Skipgram Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_8HKzdjHAPf"
   },
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        pretrained_model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        if -1 in (x1, x2): # Skip unknown words.\n",
    "            continue\n",
    "        x1, x2 = tensor(x1).to(device), tensor(x2).to(device)\n",
    "        y = w2v_io['y']\n",
    "        with torch.no_grad():\n",
    "            logprobs = pretrained_model(x1, x2)\n",
    "            _, prediction =  torch.max(logprobs, 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZLru8qVHAPg"
   },
   "outputs": [],
   "source": [
    "with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "    pretrained_keys = {line.strip():i for i, line in enumerate(fin)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJTV2BuDHAPh",
    "outputId": "8d90b9af-e011-4a4a-f499-141fdf8e62a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4996360106770201\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqYhbv64HAPi"
   },
   "source": [
    "<a id=\"section-3-1-6-eval-cbow\"></a>\n",
    "## Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5s3_QXyGHAPi"
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRnq6bWrHAPi"
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fgVYr0S1HAPj",
    "outputId": "16de0d88-8917-4514-afad-828740eddbf6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m:\u001b[0m \t\t the problem is essentially this \u001b[91mvictorino\u001b[0m if a word ( or\n",
      "\u001b[92mre-\u001b[0m \t\t the web is a vast \u001b[91mvenetians\u001b[0m source for many languages .\n",
      "\u001b[92mrandom\u001b[0m \t\t is that the association is \u001b[91mbhim\u001b[0m , arbitrary , motivated or\n",
      "\u001b[92m,\u001b[0m \t\t that the association is random \u001b[91mco-ordinate\u001b[0m arbitrary , motivated or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t arbitrary , motivated or pre- \u001b[91mmasovian\u001b[0m ( r , a ,\n",
      "\u001b[92minevitably\u001b[0m \t however , their methods are \u001b[91mrecolored\u001b[0m noisy , suffering , for\n",
      "\u001b[92mnoisy\u001b[0m \t\t , their methods are inevitably \u001b[91mmid-columbia\u001b[0m , suffering , for example\n",
      "\u001b[92m,\u001b[0m \t\t their methods are inevitably noisy \u001b[91mjailbreak\u001b[0m suffering , for example ,\n",
      "\u001b[92msuffering\u001b[0m \t methods are inevitably noisy , \u001b[91mscheepbouwer\u001b[0m , for example , from\n",
      "\u001b[92m,\u001b[0m \t\t are inevitably noisy , suffering \u001b[91mlandrum\u001b[0m for example , from just\n",
      "\u001b[92mfor\u001b[0m \t\t inevitably noisy , suffering , \u001b[91moperatives\u001b[0m example , from just those\n",
      "\u001b[92mexample\u001b[0m \t noisy , suffering , for \u001b[91morihime\u001b[0m , from just those parser\n",
      "\u001b[92m,\u001b[0m \t\t , suffering , for example \u001b[91mmid-way\u001b[0m from just those parser errors\n",
      "\u001b[92mfrom\u001b[0m \t\t suffering , for example , \u001b[91mwars\u001b[0m just those parser errors that\n",
      "\u001b[92mjust\u001b[0m \t\t , for example , from \u001b[91mocracoke\u001b[0m those parser errors that the\n",
      "\u001b[92mthose\u001b[0m \t\t for example , from just \u001b[91mobayashi\u001b[0m parser errors that the whole\n",
      "\u001b[92mparser\u001b[0m \t\t example , from just those \u001b[91munsurprising\u001b[0m errors that the whole process\n",
      "\u001b[92merrors\u001b[0m \t\t , from just those parser \u001b[91msonthi\u001b[0m that the whole process is\n",
      "\u001b[92mthat\u001b[0m \t\t from just those parser errors \u001b[91mdmd\u001b[0m the whole process is designed\n",
      "\u001b[92mthe\u001b[0m \t\t just those parser errors that \u001b[91mtravelling\u001b[0m whole process is designed to\n",
      "\u001b[92mwhole\u001b[0m \t\t those parser errors that the \u001b[91mpremonstratensian\u001b[0m process is designed to address\n",
      "\u001b[92mprocess\u001b[0m \t parser errors that the whole \u001b[91mlobo\u001b[0m is designed to address ,\n",
      "\u001b[92mis\u001b[0m \t\t errors that the whole process \u001b[91mhandle\u001b[0m designed to address , and\n",
      "\u001b[92mdesigned\u001b[0m \t that the whole process is \u001b[91mlammas\u001b[0m to address , and they\n",
      "\u001b[92mto\u001b[0m \t\t the whole process is designed \u001b[91mwell-below\u001b[0m address , and they do\n",
      "\u001b[92maddress\u001b[0m \t whole process is designed to \u001b[91mzarb\u001b[0m , and they do not\n",
      "\u001b[92m,\u001b[0m \t\t process is designed to address \u001b[91msecularised\u001b[0m and they do not wish\n",
      "\u001b[92mand\u001b[0m \t\t is designed to address , \u001b[91mgenl\u001b[0m they do not wish to\n",
      "\u001b[92mthey\u001b[0m \t\t designed to address , and \u001b[91munsurprising\u001b[0m do not wish to accept\n",
      "\u001b[92mdo\u001b[0m \t\t to address , and they \u001b[91mo'rahilly\u001b[0m not wish to accept any\n",
      "\u001b[92mnot\u001b[0m \t\t address , and they do \u001b[91mmoro\u001b[0m wish to accept any scf\n",
      "\u001b[92mwish\u001b[0m \t\t , and they do not \u001b[91mpontchartrain\u001b[0m to accept any scf for\n",
      "\u001b[92mto\u001b[0m \t\t and they do not wish \u001b[91mmidd\u001b[0m accept any scf for which\n",
      "\u001b[92maccept\u001b[0m \t\t they do not wish to \u001b[91mpalus\u001b[0m any scf for which there\n",
      "\u001b[92many\u001b[0m \t\t do not wish to accept \u001b[91mgrose\u001b[0m scf for which there is\n",
      "\u001b[92mscf\u001b[0m \t\t not wish to accept any \u001b[91mdorking\u001b[0m for which there is any\n",
      "\u001b[92mfor\u001b[0m \t\t wish to accept any scf \u001b[91mladbroke\u001b[0m which there is any evidence\n",
      "\u001b[92mwhich\u001b[0m \t\t to accept any scf for \u001b[91mstriding\u001b[0m there is any evidence as\n",
      "\u001b[92mthere\u001b[0m \t\t accept any scf for which \u001b[91mquiz\u001b[0m is any evidence as a\n",
      "\u001b[92mis\u001b[0m \t\t any scf for which there \u001b[91mfahrenheit\u001b[0m any evidence as a true\n",
      "\u001b[92many\u001b[0m \t\t scf for which there is \u001b[91mteekay\u001b[0m evidence as a true scf\n",
      "\u001b[92mevidence\u001b[0m \t for which there is any \u001b[91mjanuary-november\u001b[0m as a true scf for\n",
      "\u001b[92mas\u001b[0m \t\t which there is any evidence \u001b[91mcollaborates\u001b[0m a true scf for the\n",
      "\u001b[92ma\u001b[0m \t\t there is any evidence as \u001b[91muav\u001b[0m true scf for the verb\n",
      "\u001b[92mtrue\u001b[0m \t\t is any evidence as a \u001b[91mwosz\u001b[0m scf for the verb .\n",
      "\u001b[92mthat\u001b[0m \t\t while it might seem plausible \u001b[91mpontchartrain\u001b[0m oddities would in some way\n",
      "\u001b[92moddities\u001b[0m \t it might seem plausible that \u001b[91mfungurume\u001b[0m would in some way balance\n",
      "\u001b[92mwould\u001b[0m \t\t might seem plausible that oddities \u001b[91msampoerna\u001b[0m in some way balance out\n",
      "\u001b[92min\u001b[0m \t\t seem plausible that oddities would \u001b[91mlavera\u001b[0m some way balance out to\n",
      "\u001b[92msome\u001b[0m \t\t plausible that oddities would in \u001b[91mstover\u001b[0m way balance out to give\n",
      "\u001b[92mway\u001b[0m \t\t that oddities would in some \u001b[91mzheng\u001b[0m balance out to give a\n",
      "\u001b[92m<unk>\u001b[0m \t\t balance out to give a \u001b[91mblick\u001b[0m tion that was indistinguishable from\n",
      "\u001b[92mone\u001b[0m \t\t tion that was indistinguishable from \u001b[91mpiasecki\u001b[0m where the individual words (\n",
      "\u001b[92mwhere\u001b[0m \t\t that was indistinguishable from one \u001b[91mwelt\u001b[0m the individual words ( as\n",
      "\u001b[92mthe\u001b[0m \t\t was indistinguishable from one where \u001b[91mvalenti\u001b[0m individual words ( as opposed\n",
      "\u001b[92mindividual\u001b[0m \t indistinguishable from one where the \u001b[91melectricians\u001b[0m words ( as opposed to\n",
      "\u001b[92mwords\u001b[0m \t\t from one where the individual \u001b[91mbartram\u001b[0m ( as opposed to the\n",
      "\u001b[92m(\u001b[0m \t\t one where the individual words \u001b[91mpager\u001b[0m as opposed to the texts\n",
      "\u001b[92mas\u001b[0m \t\t where the individual words ( \u001b[91mconall\u001b[0m opposed to the texts )\n",
      "\u001b[92mopposed\u001b[0m \t the individual words ( as \u001b[91mtok'ra\u001b[0m to the texts ) had\n",
      "\u001b[92mto\u001b[0m \t\t individual words ( as opposed \u001b[91mteekay\u001b[0m the texts ) had been\n",
      "\u001b[92mthe\u001b[0m \t\t words ( as opposed to \u001b[91mblick\u001b[0m texts ) had been randomly\n",
      "\u001b[92mtexts\u001b[0m \t\t ( as opposed to the \u001b[91marche\u001b[0m ) had been randomly selected\n",
      "\u001b[92m)\u001b[0m \t\t as opposed to the texts \u001b[91mpolygons\u001b[0m had been randomly selected ,\n",
      "\u001b[92mhad\u001b[0m \t\t opposed to the texts ) \u001b[91magitator\u001b[0m been randomly selected , this\n",
      "\u001b[92mbeen\u001b[0m \t\t to the texts ) had \u001b[91memv\u001b[0m randomly selected , this turns\n",
      "\u001b[92mrandomly\u001b[0m \t the texts ) had been \u001b[91mchartres\u001b[0m selected , this turns out\n",
      "\u001b[92mselected\u001b[0m \t texts ) had been randomly \u001b[91mbarenboim\u001b[0m , this turns out not\n",
      "\u001b[92m,\u001b[0m \t\t ) had been randomly selected \u001b[91mkreuzer\u001b[0m this turns out not to\n",
      "\u001b[92mthis\u001b[0m \t\t had been randomly selected , \u001b[91mbild\u001b[0m turns out not to be\n",
      "\u001b[92mturns\u001b[0m \t\t been randomly selected , this \u001b[91mromance\u001b[0m out not to be the\n",
      "\u001b[92mout\u001b[0m \t\t randomly selected , this turns \u001b[91mphp0\u001b[0m not to be the case\n",
      "\u001b[92mnot\u001b[0m \t\t selected , this turns out \u001b[91mrekindle\u001b[0m to be the case .\n",
      "\u001b[92mvaries\u001b[0m \t\t however where the sample size \u001b[91mvictorias\u001b[0m by an order of magnitude\n",
      "\u001b[92mby\u001b[0m \t\t where the sample size varies \u001b[91mgoverning\u001b[0m an order of magnitude ,\n",
      "\u001b[92man\u001b[0m \t\t the sample size varies by \u001b[91mlammas\u001b[0m order of magnitude , or\n",
      "\u001b[92morder\u001b[0m \t\t sample size varies by an \u001b[91mstaats\u001b[0m of magnitude , or where\n",
      "\u001b[92mof\u001b[0m \t\t size varies by an order \u001b[91mpredecessor\u001b[0m magnitude , or where it\n",
      "\u001b[92mmagnitude\u001b[0m \t varies by an order of \u001b[91msalm\u001b[0m , or where it is\n",
      "\u001b[92m,\u001b[0m \t\t by an order of magnitude \u001b[91minvested\u001b[0m or where it is enormous\n",
      "\u001b[92mor\u001b[0m \t\t an order of magnitude , \u001b[91mhibiscus\u001b[0m where it is enormous ,\n",
      "\u001b[92mwhere\u001b[0m \t\t order of magnitude , or \u001b[91mwoo-suk\u001b[0m it is enormous , it\n",
      "\u001b[92mit\u001b[0m \t\t of magnitude , or where \u001b[91mshipp\u001b[0m is enormous , it is\n",
      "\u001b[92mis\u001b[0m \t\t magnitude , or where it \u001b[91mthree-mile\u001b[0m enormous , it is wrong\n",
      "\u001b[92menormous\u001b[0m \t , or where it is \u001b[91mal-bayan\u001b[0m , it is wrong to\n",
      "\u001b[92m,\u001b[0m \t\t or where it is enormous \u001b[91munsurprising\u001b[0m it is wrong to identify\n",
      "\u001b[92mit\u001b[0m \t\t where it is enormous , \u001b[91mwittman\u001b[0m is wrong to identify the\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of the conference of \u001b[91mbookrunner\u001b[0m south-central sas users group ,\n",
      "\u001b[92man\u001b[0m \t\t making false assumptions is often \u001b[91mmappings\u001b[0m ingenious way to proceed ;\n",
      "\u001b[92mingenious\u001b[0m \t false assumptions is often an \u001b[91mdespatched\u001b[0m way to proceed ; the\n",
      "\u001b[92mway\u001b[0m \t\t assumptions is often an ingenious \u001b[91m'how\u001b[0m to proceed ; the problem\n",
      "\u001b[92mto\u001b[0m \t\t is often an ingenious way \u001b[91mrimantas\u001b[0m proceed ; the problem arises\n",
      "\u001b[92mproceed\u001b[0m \t often an ingenious way to \u001b[91matoll\u001b[0m ; the problem arises where\n",
      "\u001b[92m;\u001b[0m \t\t an ingenious way to proceed \u001b[91mbunzl\u001b[0m the problem arises where the\n",
      "\u001b[92mthe\u001b[0m \t\t ingenious way to proceed ; \u001b[91mthuringowa\u001b[0m problem arises where the literal\n",
      "\u001b[92mproblem\u001b[0m \t way to proceed ; the \u001b[91mtok'ra\u001b[0m arises where the literal falsity\n",
      "\u001b[92marises\u001b[0m \t\t to proceed ; the problem \u001b[91mmystere\u001b[0m where the literal falsity of\n",
      "\u001b[92mwhere\u001b[0m \t\t proceed ; the problem arises \u001b[91mentangle\u001b[0m the literal falsity of the\n",
      "\u001b[92mthe\u001b[0m \t\t ; the problem arises where \u001b[91mgrauman\u001b[0m literal falsity of the assumption\n",
      "\u001b[92mliteral\u001b[0m \t the problem arises where the \u001b[91mtok'ra\u001b[0m falsity of the assumption is\n",
      "\u001b[92mfalsity\u001b[0m \t problem arises where the literal \u001b[91mmeow\u001b[0m of the assumption is overlooked\n",
      "\u001b[92mof\u001b[0m \t\t arises where the literal falsity \u001b[91mreigning\u001b[0m the assumption is overlooked ,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mthe\u001b[0m \t\t where the literal falsity of \u001b[91mvltava\u001b[0m assumption is overlooked , and\n",
      "\u001b[92m<unk>\u001b[0m \t\t assumption is overlooked , and \u001b[91mpalestinans\u001b[0m ate inferences are drawn .\n",
      "\u001b[92m<unk>\u001b[0m \t\t when we look at linguistic \u001b[91munsurprising\u001b[0m ena in corpora , the\n",
      "\u001b[92mnull\u001b[0m \t\t ena in corpora , the \u001b[91mnussbaum\u001b[0m hypothesis will never be true\n",
      "\u001b[92mhypothesis\u001b[0m \t in corpora , the null \u001b[91mteamed\u001b[0m will never be true .\n",
      "\u001b[92menough\u001b[0m \t\t we do not always have \u001b[91mpensioners\u001b[0m data to reject the null\n",
      "\u001b[92mdata\u001b[0m \t\t do not always have enough \u001b[91mmearns\u001b[0m to reject the null hypothesis\n",
      "\u001b[92mto\u001b[0m \t\t not always have enough data \u001b[91m'dirty\u001b[0m reject the null hypothesis ,\n",
      "\u001b[92mreject\u001b[0m \t\t always have enough data to \u001b[91mgesury\u001b[0m the null hypothesis , but\n",
      "\u001b[92mthe\u001b[0m \t\t have enough data to reject \u001b[91mfidonet\u001b[0m null hypothesis , but that\n",
      "\u001b[92mnull\u001b[0m \t\t enough data to reject the \u001b[91mbabenberg\u001b[0m hypothesis , but that is\n",
      "\u001b[92mhypothesis\u001b[0m \t data to reject the null \u001b[91mrecolored\u001b[0m , but that is a\n",
      "\u001b[92m,\u001b[0m \t\t to reject the null hypothesis \u001b[91moba\u001b[0m but that is a distinct\n",
      "\u001b[92mbut\u001b[0m \t\t reject the null hypothesis , \u001b[91mdistrustful\u001b[0m that is a distinct issue\n",
      "\u001b[92mthat\u001b[0m \t\t the null hypothesis , but \u001b[91mmaroc\u001b[0m is a distinct issue :\n",
      "\u001b[92mis\u001b[0m \t\t null hypothesis , but that \u001b[91mbinaural\u001b[0m a distinct issue : wherever\n",
      "\u001b[92ma\u001b[0m \t\t hypothesis , but that is \u001b[91meuromerchant\u001b[0m distinct issue : wherever there\n",
      "\u001b[92mdistinct\u001b[0m \t , but that is a \u001b[91mmalkin\u001b[0m issue : wherever there is\n",
      "\u001b[92missue\u001b[0m \t\t but that is a distinct \u001b[91mbelated\u001b[0m : wherever there is enough\n",
      "\u001b[92m:\u001b[0m \t\t that is a distinct issue \u001b[91mseagram\u001b[0m wherever there is enough data\n",
      "\u001b[92mwherever\u001b[0m \t is a distinct issue : \u001b[91msoir\u001b[0m there is enough data ,\n",
      "\u001b[92mthere\u001b[0m \t\t a distinct issue : wherever \u001b[91mwithered\u001b[0m is enough data , it\n",
      "\u001b[92mis\u001b[0m \t\t distinct issue : wherever there \u001b[91mavestan\u001b[0m enough data , it is\n",
      "\u001b[92menough\u001b[0m \t\t issue : wherever there is \u001b[91msuccumbs\u001b[0m data , it is rejected\n",
      "\u001b[92mdata\u001b[0m \t\t : wherever there is enough \u001b[91memesa\u001b[0m , it is rejected .\n",
      "\u001b[92mare\u001b[0m \t\t since words in a text \u001b[91mpiasecki\u001b[0m not random , we know\n",
      "\u001b[92mnot\u001b[0m \t\t words in a text are \u001b[91mphysiognomy\u001b[0m random , we know that\n",
      "\u001b[92mrandom\u001b[0m \t\t in a text are not \u001b[91mcriticised\u001b[0m , we know that our\n",
      "\u001b[92m,\u001b[0m \t\t a text are not random \u001b[91moperatives\u001b[0m we know that our corpora\n",
      "\u001b[92mwe\u001b[0m \t\t text are not random , \u001b[91mdn#\u001b[0m know that our corpora are\n",
      "\u001b[92mknow\u001b[0m \t\t are not random , we \u001b[91mquiz\u001b[0m that our corpora are not\n",
      "\u001b[92mthat\u001b[0m \t\t not random , we know \u001b[91mkreuzer\u001b[0m our corpora are not randomly\n",
      "\u001b[92mour\u001b[0m \t\t random , we know that \u001b[91mmotorcades\u001b[0m corpora are not randomly generated\n",
      "\u001b[92mcorpora\u001b[0m \t , we know that our \u001b[91maktuelt\u001b[0m are not randomly generated ,\n",
      "\u001b[92mare\u001b[0m \t\t we know that our corpora \u001b[91mmueang\u001b[0m not randomly generated , and\n",
      "\u001b[92mnot\u001b[0m \t\t know that our corpora are \u001b[91mwattoo\u001b[0m randomly generated , and the\n",
      "\u001b[92mrandomly\u001b[0m \t that our corpora are not \u001b[91meraiba\u001b[0m generated , and the hypothesis\n",
      "\u001b[92mgenerated\u001b[0m \t our corpora are not randomly \u001b[91mjew\u001b[0m , and the hypothesis test\n",
      "\u001b[92mthe\u001b[0m \t\t gives us reason to view \u001b[91mcoulthard\u001b[0m relation between , for example\n",
      "\u001b[92mrelation\u001b[0m \t us reason to view the \u001b[91mlondinium\u001b[0m between , for example ,\n",
      "\u001b[92mbetween\u001b[0m \t reason to view the relation \u001b[91mwral\u001b[0m , for example , a\n",
      "\u001b[92m,\u001b[0m \t\t to view the relation between \u001b[91mworkup\u001b[0m for example , a verb\n",
      "\u001b[92m<unk>\u001b[0m \t\t for example , a verb \u001b[91mnewmar\u001b[0m s syntax and its semantics\n",
      "\u001b[92m,\u001b[0m \t\t s syntax and its semantics \u001b[91mmmbg\u001b[0m as motivated rather than arbitrary\n",
      "\u001b[92mas\u001b[0m \t\t syntax and its semantics , \u001b[91mlocative\u001b[0m motivated rather than arbitrary .\n",
      "\u001b[92merror\u001b[0m \t\t the average value of the \u001b[91mdrastically\u001b[0m term , language is never\n",
      "\u001b[92mterm\u001b[0m \t\t average value of the error \u001b[91mullevi\u001b[0m , language is never ,\n",
      "\u001b[92m,\u001b[0m \t\t value of the error term \u001b[91mblick\u001b[0m language is never , ever\n",
      "\u001b[92mlanguage\u001b[0m \t of the error term , \u001b[91mrecolored\u001b[0m is never , ever ,\n",
      "\u001b[92mis\u001b[0m \t\t the error term , language \u001b[91mblick\u001b[0m never , ever , ever\n",
      "\u001b[92mnever\u001b[0m \t\t error term , language is \u001b[91mmaru\u001b[0m , ever , ever ,\n",
      "\u001b[92m,\u001b[0m \t\t term , language is never \u001b[91mantananarivo\u001b[0m ever , ever , random\n",
      "\u001b[92m,\u001b[0m \t\t the hypothesis can , therefore \u001b[91msteamship\u001b[0m be couched as : are\n",
      "\u001b[92mbe\u001b[0m \t\t hypothesis can , therefore , \u001b[91msolder\u001b[0m couched as : are the\n",
      "\u001b[92mcouched\u001b[0m \t can , therefore , be \u001b[91mlammas\u001b[0m as : are the error\n",
      "\u001b[92mas\u001b[0m \t\t , therefore , be couched \u001b[91m----\u001b[0m : are the error terms\n",
      "\u001b[92m:\u001b[0m \t\t therefore , be couched as \u001b[91mcos0\u001b[0m are the error terms systematically\n",
      "\u001b[92mare\u001b[0m \t\t , be couched as : \u001b[91mreflex\u001b[0m the error terms systematically greater\n",
      "\u001b[92mthe\u001b[0m \t\t be couched as : are \u001b[91mmoro\u001b[0m error terms systematically greater than\n",
      "\u001b[92mbecomes\u001b[0m \t % of them , devastate \u001b[91mitsekiri\u001b[0m one of the verbs for\n",
      "\u001b[92mone\u001b[0m \t\t of them , devastate becomes \u001b[91mteekay\u001b[0m of the verbs for which\n",
      "\u001b[92mof\u001b[0m \t\t them , devastate becomes one \u001b[91mrecolored\u001b[0m the verbs for which we\n",
      "\u001b[92mthe\u001b[0m \t\t , devastate becomes one of \u001b[91minterplanetary\u001b[0m verbs for which we have\n",
      "\u001b[92mverbs\u001b[0m \t\t devastate becomes one of the \u001b[91mhankook\u001b[0m for which we have plenty\n",
      "\u001b[92mfor\u001b[0m \t\t becomes one of the verbs \u001b[91mmosenergo\u001b[0m which we have plenty of\n",
      "\u001b[92mwhich\u001b[0m \t\t one of the verbs for \u001b[91mstover\u001b[0m we have plenty of data\n",
      "\u001b[92mwe\u001b[0m \t\t of the verbs for which \u001b[91mnarbonensis\u001b[0m have plenty of data ,\n",
      "\u001b[92mhave\u001b[0m \t\t the verbs for which we \u001b[91mbloch\u001b[0m plenty of data , and\n",
      "\u001b[92mplenty\u001b[0m \t\t verbs for which we have \u001b[91msolti\u001b[0m of data , and crude\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x']).to(device)\n",
    "        y = tensor(w2v_io['y']).to(device)\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRmLeM9-HAPk",
    "outputId": "2a727ef6-0d7a-42d2-81c3-258493823af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPYRyVP4HAPl"
   },
   "source": [
    "<a id=\"section-3-1-6-unfreeze-finetune\"></a>\n",
    "## Unfreeze the Embedddings and Tune it on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4cUE4nTHAPl",
    "outputId": "ec962a3c-d9c8-4abe-b7fd-2849e17e150d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a461e43c6728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_npy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCBOW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membd_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_npy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# See https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        # See https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding.from_pretrained\n",
    "        # Note the `freeze=False`, by default if you use `nn.Embedding.from_pretrained(),\n",
    "        # `freeze` is set to True\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy, \n",
    "                                                       freeze=False)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfwppYi8HAPm",
    "outputId": "c31a71a9-9f22-4391-f102-cc3ef672ce45"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2VecText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-985992808650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpretrained_cbow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_npy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Word2VecText' is not defined"
     ]
    }
   ],
   "source": [
    "window_size = 2 \n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpQbKw8iHAPm",
    "outputId": "7eb62122-8c66-4444-be79-831edb3fbd97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-4f84802592bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# This unsqueeze thing is really a feature/bug... -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(pretrained_cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(pretrained_cbow_model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x = tensor(w2v_io['x']).to(device)\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long)).to(device)\n",
    "            \n",
    "            if -1 in x or int(y) == -1:\n",
    "                continue\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = pretrained_cbow_model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_finetuning_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTKWhnDXHAPn"
   },
   "source": [
    "<a id=\"section-3-1-6-reval-cbow\"></a>\n",
    "\n",
    "## Re-Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKmkIvpeHAPn"
   },
   "outputs": [],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x']).to(device)\n",
    "        y = tensor(w2v_io['y']).to(device)\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtTOg58JHAPo",
    "outputId": "17789194-c928-4e7f-88d6-892e0af1b30c"
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ang2rfTtHAPp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRGV62gLHAPq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7wLms84nHlmc",
    "4euuHE2ZHlmg",
    "dNiCSpQjHlmu",
    "xFyT-2C7Hlmx",
    "NfiJsVXfHlne",
    "JljjG8wPHlnq",
    "g5FFlCOxHloZ",
    "eJZPBd_eHloc",
    "SjnIlUDeHlpH",
    "_Lcv5kLPHlpW",
    "YwizIFgnHlpf",
    "UD-HOa8AHlpo",
    "opZKKtIKHlpu",
    "v3oQzqhoHlpv",
    "mabiO5n1Hlpx",
    "TouRhGbDHlp5",
    "40WxV7F6HlqD",
    "G02TvHoqHlqD",
    "QQbNKqJBHlqG",
    "5Ps4hXZPHlqK",
    "wkMi05XVHlqR",
    "bxSp00OXHlqc",
    "E6QkNPykHlqm"
   ],
   "name": "Session3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
